contents

  * [mini-batch gradient descent](#mini-batch-gradient-descent)
  * [understanding mini-batch gradient descent](#understanding-mini-batch-gradient-descent)
  * [exponentially weighted averages](#exponentially-weighted-averages)
  * [understanding exponentially weighted averages](#understanding-exponentially-weighted-averages)
  * [bias correction in exponentially weighted averages](#bias-correction-in-exponentially-weighted-averages)
  * [gradient descent with momentum](#gradient-descent-with-momentum)
  * [RMSprop](#rmsprop)
  * [Adam optimization algorithm](#adam-optimization-algorithm)
  * [learning rate decay](#learning-rate-decay)
  * [the problem of local optima](#the-problem-of-local-optima)

## mini-batch gradient descent

+ batch gradient descent：遍历所有训练样本
+ mini-batch gradient descent：一次处理一个min-batch

各符号的表示：

+ $X^{(i)}$表示第i个训练样本
+ $z^{[l]}$表求第l层的激活函数的输入
+ $X^{\{t \}},Y^{\{t \}}$表示第t个mini-batch的样本

![mini-batch-gradient-descent-introduction.png](https://raw.githubusercontent.com/daiwk/dl.ai/master/c2/imgs/mini-batch-gradient-descent-introduction.png)

假设有5000000个样本，那么在一个epoch中，遍历5000个mini-batch，每个mini-batch（1000个样本）中，先前向计算，然后算这1000个样本的loss，然后反向传播。

注意：一个mini-batch就反射传播一次，更新一次参数，所以一个epoch就更新了5000次参数。

![mini-batch-gradient-descent.png](https://raw.githubusercontent.com/daiwk/dl.ai/master/c2/imgs/mini-batch-gradient-descent.png)

## understanding mini-batch gradient descent

可见，使用mini-batch的cost会有不小的抖动和噪音。

![mini-batch-gradient-descent-cost.png](https://raw.githubusercontent.com/daiwk/dl.ai/master/c2/imgs/mini-batch-gradient-descent-cost.png)

+ batchsize=m: batch gradient descent。一般可以顺利到达min。但耗时长。
+ batchsize=1: stochastic gradient descent。很可能在min附近徘徊。而且失去了vectorization可以带来了加速
+ batchsize between 1,m: 
  + 充分利用vectorization
  + make progress without processing entire training set

![mini-batch-gradient-descent-choose-batchsize.png](https://raw.githubusercontent.com/daiwk/dl.ai/master/c2/imgs/mini-batch-gradient-descent-choose-batchsize.png)

+ 训练集很小时（m<=2000）：使用batch gradient descent
+ 其他情况下，经典的batchsize：2的指数（64,128,256,512），一般比较少用1024，非2的指数也比较少用（如1000）
+ make sure mini-batch里的$X^{\{t\}},Y^{\{t\}}$能够在cpu/gpu中被容纳下

![mini-batch-gradient-descent-choose-size.png](https://raw.githubusercontent.com/daiwk/dl.ai/master/c2/imgs/mini-batch-gradient-descent-choose-size.png)

## exponentially weighted averages

比gradient descent更快的算法大多使用指数平滑类的算法。通过图中$v_t$的计算，可以得到图中的红线，即exponentially weighted (moving) average

![exponential-weight-average-introduction.png](https://raw.githubusercontent.com/daiwk/dl.ai/master/c2/imgs/exponential-weight-average-introduction.png)

$$v_t=\beta v_{t-1}+(1-\beta)\theta_t$$

**注意，$\beta$对应的是$v_{t-1}$，而$1-\beta$对应的是$\theta_t$。**

而，曲线描绘了过去大约$\frac{1}{1-\beta}$天的变量取值的平均情况，所以，$\beta$越大，窗口越大，曲线越滞后，对变量的变化也越不敏感。图中，红线是0.9[相当于过去10天]，绿线是0.98[相当于过去50天]，黄线是0.5[相当于过去2天]。

![exponential-weight-average-formula.png](https://raw.githubusercontent.com/daiwk/dl.ai/master/c2/imgs/exponential-weight-average-formula.png)

## understanding exponentially weighted averages

$(1-\epsilon)^{1/\epsilon}$约等于$1/e$，而$\epsilon=1-\beta$。所以是${\beta}^{1/(1-\beta)}$，所以前面提到，大约是$1/(1-\beta)$天

![exponential-weight-average-analyze.png](https://raw.githubusercontent.com/daiwk/dl.ai/master/c2/imgs/exponential-weight-average-analyze.png)

好处：
+ 计算简单
+ 省内存（只需要存一个变量v就行了）

![exponential-weight-average-implementation.png](https://raw.githubusercontent.com/daiwk/dl.ai/master/c2/imgs/exponential-weight-average-implementation.png)

## bias correction in exponentially weighted averages

当$\beta=0.98$时理论上应该得到绿线，但实际得到的往往是紫线（因为v初始是0，所以而$\beta$接近1，所以起始的几个值都比较小，所以起点比较低），通过bias correction可以从紫线纠正到绿线。

$v_t=\frac{v_t}{1-\beta^t}$，因为$0<\beta<1$，所以t越大，$\beta^t$越小，分母越大，所以$v_t$越小。反之，t越小，$v_t$越大，就可以补上前面提到的t比较小时较低的起点。

![exponential-weight-average-bias-correction.png](https://raw.githubusercontent.com/daiwk/dl.ai/master/c2/imgs/exponential-weight-average-bias-correction.png)

## gradient descent with momentum

如果learning rate太大，可能出现紫线的情况，波动太大（oscillation，振动，波动）。所以，希望纵向slower，而橫向faster。

所谓momentum，就是对dW和db，分别使用exponentially weighted average，计算对应的$v_{dW}$和$v_{db}$，更新时，用这两个v替换掉原来的dW和db。

这样，能够在横向上保持方向不变，而在纵向上，有『平滑』的效果，减少波动。

公式中，$dW$相当于加速度，$v_{t}$相当于速度(velocity)，$\beta$相当于摩擦力（friction）

![momentum-introduction.png](https://raw.githubusercontent.com/daiwk/dl.ai/master/c2/imgs/momentum-introduction.png)

实现时的几个注意点：

+ 最常取的$\beta=0.9$，也就是相当于最近10个iterations的gradients。
+ 而且不一定会用bias correction($\frac{v_{dW}}{1-\beta^t}$)，因为10个iteration左右，这个bias就差不多没了。
+ 有的实现是去掉$(1-\beta)$（图中的紫色部分），而这相当于$v_{dW}$缩小了$(1-\beta)$倍，所以更新时，$\alpha$相当于要根据$\frac{1}{1-\beta}$进行相应的变化。这两种方法其实都行，但会影响到如何选择最佳的$\alpha$。而这种方法可能有一些less intuitive的就是，最后要调整$\beta$时，会影响到$v_{dW}$和$v_{db}$的scaling，从而，可能还要再修改学习率$\alpha$。

## RMSprop



## Adam optimization algorithm

## learning rate decay

## the problem of local optima
