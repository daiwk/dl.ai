contents

* [logistic regression as a neural network](#logistic-regression-as-a-neural-network)
  * [binary classification](#binary-classification)
  * [logistic regression](#logistic-regression)
  * [logistic regression cost function](#logistic-regression-cost-function)
  * [gradient descent](#gradient-descent)
  * [derivatives](#derivatives)
  * [more derivative examples](#more-derivative-examples)
  * [computation graph](#computation-graph)
  * [derivatives with a computation graph](#derivatives-with-a-computation-graph)
  * [logistic regression gradient descent](#logistic-regression-gradient-descent)
  * [gradient descent on m examples](#gradient-descent-on-m-examples)
* [python &amp; vectorization](#python--vectorization)
  * [vectorization](#vectorization)
  * [more examples of vectorization](#more-examples-of-vectorization)
  * [vectorizing logistic regression](#vectorizing-logistic-regression)
  * [vectorizing logistic regression's gradient output](#vectorizing-logistic-regressions-gradient-output)
  * [broadcasting in python](#broadcasting-in-python)
  * [a note on python/numpy vectors](#a-note-on-pythonnumpy-vectors)
  * [quick tour of jupyter/ipython notebooks](#quick-tour-of-jupyteripython-notebooks)
  * [explanation of logistic regression cost function](#explanation-of-logistic-regression-cost-function)


# logistic regression as a neural network

## binary classification

维度为(64, 64, 3)的图片 ===> img vector: x=维度为(64\*64\*3=12288, 1)的列向量。($n_x=12288$)

$$ (x,y), x \in R^{n_x}, y \in \{0,1\} $$
$m=m_{train}$个训练样本：${(x^{(1)}, y^{(1)}), ..., (x^{(m)}, y^{(m)})}, m_{test}$个测试样本。

$X$表示一个$n_x\*m$的训练样本矩阵,在python里就是```X.shape=(n_x,m)```
$Y$表示一个$1\*m$的向量,在python里是```Y.shape=(1,m)```

## logistic regression

given $x$, want $\hat y=P(y=1|x), x \in R^{n_x}$

params: $w \in R^{n_x}, b \in R$

output: $\hat y= \sigma(w^Tx+b), \sigma(z)=\frac{1}{1+e^(-z)}$

## logistic regression cost function

$(x^(i),y^(i))$ 表示第i个样本。

**Loss(error) function只针对一条训练样本：**

+ square error的loss function:
$$L(\hat y, y)=1/2*(\hat y - y)^2$$
+ logistic regression的loss function: 
$$L(\hat y, y)=-(ylog\hat y+(1-y)log(1-\hat y))$$

if $y=1, L(\hat y, y) = -log\hat y $, want $log\hat y$ large, want $\hat y$ large

if $y=0, L(\hat y, y) = -log(1-\hat y) $, want $\hat y$ small

**Cost function针对全体训练样本:**
$$J(w,b)=1/m\sum ^m_{i=1}L(\hat y^{(i)}, y^{(i)})=-1/m\sum^m_{i=1}[y^{(i)}log\hat y^{(i)}+(1-y^{(i)})log(1-\hat y^{(i)})]$$

## gradient descent

lr的$J(w,b)$是一个凸函数，所以有全局最优。 因为有全局最优，所以lr的初始化一般是0，不用随机。梯度下降：不断重复$ w=w-\alpha \frac{dJ(w)}{dw}$直到收敛。后续，用$dw$来指代$\frac{dJ(w)}{dw}$。梯度下降的公式：
$$w=w-\alpha dw$$
$$b=b-\alpha db$$

## derivatives

derivative = slope，就是$dy/dx=\Delta y/\Delta x$

## more derivative examples

$f(a)=log_e(a)=ln(a), df(a)/da=\frac{1}{a}$

## computation graph

正向计算图算出输出，算每个参数的梯度就反向算。

![computation graph](https://raw.githubusercontent.com/daiwk/dl.ai/master/c1/imgs/computation%20graph.png)

## derivatives with a computation graph
$$J=3v,v=a+u,u=bc$$
$$\frac {dJ}{dv}=3, \frac{dv}{da}=1$$
$$so, \frac {dJ}{da}=\frac {dJ}{dv}\frac {dv}{da}=3\*1=3$$
$$if\ b=2,then\ \frac{dJ}{dc}=\frac{dJ}{dv}\frac{dv}{du}\frac{du}{dc}=3\*1\*b=3\*1\*2=6$$
**写代码时，将$\frac{dFinalOutputVar}{dvar}$记为$dvar$(最后输出对这个变量的偏导)**

![derivatives in computation graph](https://raw.githubusercontent.com/daiwk/dl.ai/master/c1/imgs/computation%20graph-derivatives.png)

## logistic regression gradient descent

$$z=w^Tx+b$$
$$/hat y=a=\sigma (z)$$
$$L(a,y)=-(ylog(a)+(1-y)log(1-a))$$

![derivatives in computation graph in lr](https://raw.githubusercontent.com/daiwk/dl.ai/master/c1/imgs/computation%20graph-derivatives-lr.png)

## gradient descent on m examples

首先，根据J的公式，可以知道dJ/dw1其实就是对每个样本的dw1求和，然后/m。

![gradient_descent_lr_m_examples_djdw](https://raw.githubusercontent.com/daiwk/dl.ai/master/c1/imgs/gradient_descent_lr_m_examples_djdw.png)

每一次迭代，遍历m个样本，算出J/dw1/dw2/db，然后用这些梯度去更新一次w1/w2/b。

![gradient_descent_lr_m_examples](https://raw.githubusercontent.com/daiwk/dl.ai/master/c1/imgs/gradient_descent_lr_m_examples.png)

但这的for loop太多了。。所以我们需要vectorization!

# python & vectorization

## vectorization

![gradient_descent_lr_m_examples](https://raw.githubusercontent.com/daiwk/dl.ai/master/c1/imgs/vectorization-1.png)

对于两个100w维的向量进行点乘，vectorization(1.5ms) 比for loop(470ms+)快

## more examples of vectorization

![gradient_descent_lr_m_examples](https://raw.githubusercontent.com/daiwk/dl.ai/master/c1/imgs/vectorization-2.png)

![gradient_descent_lr_m_examples](https://raw.githubusercontent.com/daiwk/dl.ai/master/c1/imgs/vectorization-3.png)

![gradient_descent_lr_m_examples](https://raw.githubusercontent.com/daiwk/dl.ai/master/c1/imgs/vectorization-lr.png)
如上图，将`\(n_x\)`维的dw变为一个np.array即可干掉内层的for loop。

## vectorizing logistic regression

可见，整个求`\(Z\)`的过程可以变成一句话，而求A时，需要封装一个基于numpy的sigmoid函数。

![gradient_descent_lr_m_examples](https://raw.githubusercontent.com/daiwk/dl.ai/master/c1/imgs/vectorization-lr-2.png)

## vectorizing logistic regression's gradient output

![gradient_descent_lr_m_examples](https://raw.githubusercontent.com/daiwk/dl.ai/master/c1/imgs/vectorization-lr-3.png)

![gradient_descent_lr_m_examples](https://raw.githubusercontent.com/daiwk/dl.ai/master/c1/imgs/vectorization-lr-4.png)

## broadcasting in python

```
A = ndarray([[1,2,3,4],[2,3,4,5],[3,4,5,6]]) # 3*4
calc = A.sum(axis=0) # A的每列求和,得到1*4
calc2 = A.sum(axis=1) # A的每行求和,得到3*1
A/calc.reshape(1,4) #得到一个3*4的矩阵，就是broadcasting。其实等价于A/calc，但为了保险，可以调用reshape(1,4)来确保无误
```
小结：

![broadcasting](https://raw.githubusercontent.com/daiwk/dl.ai/master/c1/imgs/broadcasting.png)

## a note on python/numpy vectors

## quick tour of jupyter/ipython notebooks

## explanation of logistic regression cost function


