contents

<!-- TOC -->

- [1. deep neural network](#1-deep-neural-network)
    - [1.1. deep L-layer neural network](#11-deep-l-layer-neural-network)
    - [1.2. forward propagation in a deep network](#12-forward-propagation-in-a-deep-network)
    - [1.3. getting your matrix dimensions right](#13-getting-your-matrix-dimensions-right)
    - [1.4. why deep representations?](#14-why-deep-representations)
    - [1.5. building blocks of deep neural networks](#15-building-blocks-of-deep-neural-networks)
    - [1.6. forward and backward propagation](#16-forward-and-backward-propagation)
    - [1.7. parameters vs hyperparameters](#17-parameters-vs-hyperparameters)
    - [1.8. what does this have to do with the brain?](#18-what-does-this-have-to-do-with-the-brain)
    - [1.9. others](#19-others)

<!-- /TOC -->
# 1. deep neural network
## 1.1. deep L-layer neural network

![deep-neural-network-notation.png](../c1/imgs/deep-neural-network-notation.png)

## 1.2. forward propagation in a deep network

$z^{[l](i)}$表示第l层的第i个训练样本(列向量)，$Z^{[l]}$表示将第l层的这$m$个训练样本全部水平地放在一起形成的矩阵。以此vectorization的方法，可以避免从1->$m$的这个for-loop。

另外，从第1层到第4层这个for-loop是无法避免的。

![forward-propagation-in-a-deep-network.png](../c1/imgs/forward-propagation-in-a-deep-network.png)

## 1.3. getting your matrix dimensions right

![w-and-b-dimension.png](../c1/imgs/w-and-b-dimension.png)

vectorized之后，$b^{[l]}$仍然是$(n^{[l]},1)$维，只是因为broadcasting，才复制m遍，变成了$(n^{[l]},m)$维。

![vectorized-implementation.png](../c1/imgs/vectorized-implementation.png)

## 1.4. why deep representations?

![intuition-of-deep-representation.png](../c1/imgs/intuition-of-deep-representation.png)

对于$(x_1,x_2,...x_n)$的异或（XOR）操作，如果用树型结构，n个叶子节点，则树深度是$log_2n+1$(深度为k的满二叉树的第i层上有$2^{i-1}$个结点,总共至多有$2^k-1$个结点)，即只需要$O(log_2n)$层的树就能完成。

而如果采用单隐层，则需要$2^{(n-1)}$个节点

![circuit-theory-and-deep-learning.png](../c1/imgs/circuit-theory-and-deep-learning.png)

## 1.5. building blocks of deep neural networks

forward时，需要cache $Z^{[l]}$以供backward使用。

![forward-and-backward-functions.png](../c1/imgs/forward-and-backward-functions.png)

为了计算backward，其实需要cache的有$Z^{[l]}$、$W^{[l]}$以及$b^{[l]}$：

![forward-backward-functions.png](../c1/imgs/forward-backward-functions.png)

## 1.6. forward and backward propagation

forward propagation for layer l:

![forward-propagation-for-layer-l.png](../c1/imgs/forward-propagation-for-layer-l.png)

backward propagation for layer l:

参考[C1W2的backward propagation intuition部分](https://github.com/daiwk/dl.ai/blob/master/c1/c1w3.md#backpropagation-intuition)：
注意：$da^{[l]}*g^{[l]'}(z^{[l]})$是element-wise product。

![backward-propagation-for-layer-l.png](../c1/imgs/backward-propagation-for-layer-l.png)

对于最后一层L，如果是sigmoid并采用logloss，那么：

![summary-forward-backward-propagation.png](../c1/imgs/summary-forward-backward-propagation.png)

## 1.7. parameters vs hyperparameters

图中下方是：momentum，mini-batch size，regularization
![summary-forward-backward-propagation.png](../c1/imgs/parameters-vs-hyperparameters.png)

![apply-deep-learning-is-a-very-empirical-process.png](../c1/imgs/apply-deep-learning-is-a-very-empirical-process.png)

## 1.8. what does this have to do with the brain?

![summary-and-brain.png](../c1/imgs/summary-and-brain.png)

## 1.9. others

![shape-of-L-layer-nn.png](../c1/imgs/shape-of-L-layer-nn.png)
