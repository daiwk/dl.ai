contents

  * [neural networks overview](#neural-networks-overview)
  * [neural network representation](#neural-network-representation)
  * [computing a neural network's output](#computing-a-neural-networks-output)
  * [vectorizing across multiple examples](#vectorizing-across-multiple-examples)
  * [explanation for vectorized implementation](#explanation-for-vectorized-implementation)
  * [activation functions](#activation-functions)
  * [why do you need non-linear activation functions](#why-do-you-need-non-linear-activation-functions)
  * [gradient descent for neural networks](#gradient-descent-for-neural-networks)
  * [backpropagation intuition](#backpropagation-intuition)
  * [random initialization](#random-initialization)


## neural networks overview

其中，每个神经元完成了$z=w^Tx+b$以及$a=\sigma (z)$两个操作($a$表示activation)，每一层的数据用上标[i]表示。

![neural-networks-overview](https://raw.githubusercontent.com/daiwk/dl.ai/master/c1/imgs/neural-networks-overview.png)

## neural network representation

图示是一个2层nn（inputlayer不算在内，有hidden和output两层）。

如果输入的x有3维，在lr中，$shape(w)=(1,3)$，$shape(b)=(1,1)$。

而在nn中，$shape(w^{[1]})=(4,3)$因为有4个神经元，输入是3维。同理$shape(b^{[1]})=(4,1)$。

而$shape(w^{[2]})=(1,4)$，因为只有1个神经元，输入是3维。同理$shape(b^{[2]})=(1,1)$。

![neural-network-representation](https://raw.githubusercontent.com/daiwk/dl.ai/master/c1/imgs/neural-network-representation.png)

## computing a neural network's output

![compute-nn-output-1](https://raw.githubusercontent.com/daiwk/dl.ai/master/c1/imgs/compute-nn-output-1.png)
![compute-nn-output-1](https://raw.githubusercontent.com/daiwk/dl.ai/master/c1/imgs/compute-nn-output-2.png)

## vectorizing across multiple examples

![vectorizing-across-multiple-examples-for-loop](https://raw.githubusercontent.com/daiwk/dl.ai/master/c1/imgs/vectorizing-across-multiple-examples-for-loop.png)

矩阵$X$纵向是x的维数（行数），横向是training examples的个数（列数）。

矩阵$Z$、$A$纵向是hidden units的个数（行数），横向是training examples的个数（列数）。

![vectorizing-across-multiple-examples-vectorization](https://raw.githubusercontent.com/daiwk/dl.ai/master/c1/imgs/vectorizing-across-multiple-examples-vectorization.png)

## explanation for vectorized implementation

![justification-for-vectorized-implementation](https://raw.githubusercontent.com/daiwk/dl.ai/master/c1/imgs/justification-for-vectorized-implementation.png)
![vectorizing-across-multiple-examples-recap](https://raw.githubusercontent.com/daiwk/dl.ai/master/c1/imgs/vectorizing-across-multiple-examples-recap.png)

## activation functions

一般来说，$tanh$效果比$sigmoid$好，因为均值是0。但对于outputlayer而言，一般$y \in \\{0,1\\}$，所以希望$0\le \hat y \le 1$，所以会用$sigmoid$。

$ReLU(z)=max(0,z)$比$tanh$好，因为当$x\le 0$时，梯度为0。而$leaky ReLU$在$x\le 0$时，梯度是接近0，效果会好一点，但在实践中还是$ReLU$居多。当$x>0$时，梯度和$x\le0$差很多，所以训练速度会加快。

![activation-functions](https://raw.githubusercontent.com/daiwk/dl.ai/master/c1/imgs/activation-functions.png)

## why do you need non-linear activation functions

linear activation: 因为$z=wx+b$，所以激活函数$g(z)=z=wx+b$就叫linear activation function，也叫identity activatioin function。

不要在hidden layer用linear activation functions，因为多个linear嵌套，实质上还是linear。

例外，当进行回归时，$y\inR$，可以hidden layer用$ReLU$，但output layer用linear activation。

![activation-functions](https://raw.githubusercontent.com/daiwk/dl.ai/master/c1/imgs/activation-functions.png)

## gradient descent for neural networks

## backpropagation intuition

## random initialization


