contents

- [setting up your machine learning application](#setting-up-your-machine-learning-application)
  - [train/dev/test sets](#traindevtest-sets)
  - [bias/variance](#biasvariance)
  - [basic recipe for machine learning](#basic-recipe-for-machine-learning)
- [regularizing your neural network](#regularizing-your-neural-network)
  - [regularization](#regularization)
  - [why regularization reduces overfitting?](#why-regularization-reduces-overfitting)
  - [dropout regularization](#dropout-regularization)
  - [understanding dropout](#understanding-dropout)
  - [other regularization methods](#other-regularization-methods)
    - [data augmentation](#data-augmentation)
    - [early stopping](#early-stopping)
- [setting up your optimization problem](#setting-up-your-optimization-problem)
  - [normalizing inputs](#normalizing-inputs)
  - [vanishing/exploding gradients](#vanishingexploding-gradients)
  - [weight initialization for deep networks](#weight-initialization-for-deep-networks)
  - [numerical approximation of gradients](#numerical-approximation-of-gradients)
  - [gradient checking](#gradient-checking)
  - [gradient checking implementation notes](#gradient-checking-implementation-notes)

# setting up your machine learning application

## train/dev/test sets

ä¼ ç»Ÿæœºå™¨å­¦ä¹ ï¼Œä¾‹å¦‚æ•°æ®æ€»é‡æœ‰1wï¼Œå¯ä»¥åˆ’åˆ†train:dev:test=70:0:30ï¼Œæˆ–è€…ï¼Œtrain:dev:test=60:20:20ã€‚
ä½†å¯¹äºå¤§æ•°æ®ï¼Œä¾‹å¦‚100wçš„æ•°æ®ï¼Œé‚£é€‚å½“çš„æ¯”ä¾‹åº”è¯¥æ˜¯98:1:1æˆ–è€…ç”šè‡³æ˜¯99.5:0.25:0.25ï¼Œæˆ–è€…99.5:0.4:0.1ã€‚

![train-dev-test-ratio.png](https://raw.githubusercontent.com/daiwk/dl.ai/master/c2/imgs/train-dev-test-ratio.png)

å¦å¤–ï¼Œè®­ç»ƒé›†å’ŒéªŒè¯é›†çš„åˆ†å¸ƒè¦ä¿æŒä¸€è‡´ã€‚æµ‹è¯•é›†æ˜¯ä¸ºäº†no-biasåœ°éªŒè¯æ¨¡å‹æ•ˆæœçš„ï¼Œæœ‰äº›æ—¶å€™ï¼Œå¯ä»¥ç”šè‡³ä¸è¦æµ‹è¯•é›†ï¼Œåªè¦éªŒè¯é›†å°±å¯ä»¥äº†ã€‚

![distribution-between-train-and-dev.png](https://raw.githubusercontent.com/daiwk/dl.ai/master/c2/imgs/distribution-between-train-and-dev.png)

## bias/variance
![bias-variance-under-overfitting.png](https://raw.githubusercontent.com/daiwk/dl.ai/master/c2/imgs/bias-variance-under-overfitting.png)
é¦–å…ˆ Error = Bias + Variance

Erroråæ˜ çš„æ˜¯æ•´ä¸ªæ¨¡å‹çš„å‡†ç¡®åº¦ï¼ŒBiasåæ˜ çš„æ˜¯æ¨¡å‹åœ¨æ ·æœ¬ä¸Šçš„è¾“å‡ºä¸çœŸå®å€¼ä¹‹é—´çš„è¯¯å·®ï¼Œå³æ¨¡å‹æœ¬èº«çš„ç²¾å‡†åº¦ï¼ŒVarianceåæ˜ çš„æ˜¯æ¨¡å‹æ¯ä¸€æ¬¡è¾“å‡ºç»“æœä¸æ¨¡å‹è¾“å‡ºæœŸæœ›ä¹‹é—´çš„è¯¯å·®ï¼Œå³æ¨¡å‹çš„ç¨³å®šæ€§ã€‚[https://www.zhihu.com/question/27068705/answer/35151681](https://www.zhihu.com/question/27068705/answer/35151681)

low-high-variance-biasçš„å››è±¡é™å¦‚ä¸‹ï¼š

![bias-variance-high-low.png](https://raw.githubusercontent.com/daiwk/dl.ai/master/c2/imgs/bias-variance-high-low.png)

å¯¹åº”æ¨¡å‹çš„æœ€ä¼˜å¤æ‚åº¦å¦‚ä¸‹ï¼š
![bias-variance-high-low-complexity.png](https://raw.githubusercontent.com/daiwk/dl.ai/master/c2/imgs/bias-variance-high-low-complexity.png)

å½“optimal error(bayes error)çº¦ç­‰äº0%æ—¶ï¼ˆé”™è¯¯ç‡æ˜¯0ï¼Œå…¨éƒ¨æ‰èƒ½è¯†åˆ«å‡ºæ¥ï¼‰ï¼Œå¦‚ä¸‹å›¾æ‰€ç¤ºã€‚å½“optimal erroræ˜¯15%æ—¶ï¼Œç¬¬äºŒä¸ªåˆ†ç±»å™¨å°±æ˜¯low biasäº†ã€‚

![bias-variance-cat-classification.png](https://raw.githubusercontent.com/daiwk/dl.ai/master/c2/imgs/bias-variance-cat-classification.png)

æœ€å·®æƒ…å†µå°±æ˜¯ä¸¤ä¸ªéƒ½highçš„ï¼Œå¦‚ä¸‹ç´«è‰²æ›²çº¿ï¼ˆçº¿æ€§éƒ¨åˆ†æ˜¯high biasï¼Œå› ä¸ºæœ‰å¾ˆå¤šæ²¡åˆ†å¯¹çš„æƒ…å†µï¼›ä¸­é—´ä¸¤ä¸ªç‚¹æ˜¯high varianceï¼Œoverfittingäº†ï¼‰:

![bias-variance-worst.png](https://raw.githubusercontent.com/daiwk/dl.ai/master/c2/imgs/bias-variance-worst.png)

## basic recipe for machine learning

é¦–å…ˆï¼Œå¦‚æœä½ çš„æ¨¡å‹æœ‰high bias(åœ¨è®­ç»ƒé›†ä¸Šè¡¨ç°å¾ˆå·®)ï¼Œé‚£ä¹ˆï¼Œè¯•ç€ç”¨æ›´å¤§çš„ç½‘ç»œï¼ˆæ›´å¤æ‚çš„æ¨¡å‹ï¼‰ï¼Œæˆ–è€…ï¼Œè®­ç»ƒæ›´ä¹…ã€‚

å¦‚æœbiaså·²ç»å˜å°äº†ï¼Œé‚£ä¹ˆçœ‹çœ‹æ˜¯å¦æ˜¯high viarance(ä»è®­ç»ƒé›†åˆ°éªŒè¯é›†çš„è¡¨ç°çš„å˜åŒ–)ï¼Œå¦‚æœæ˜¯ï¼Œé‚£ä¹ˆå¯ä»¥å°è¯•è·å–æ›´å¤šæ•°æ®/æ­£åˆ™åŒ–ã€‚

åœ¨ä¼ ç»Ÿæœºå™¨å­¦ä¹ é¢†åŸŸï¼Œæœ‰bias-variance tradeoffçš„è¯´æ³•ï¼Œå› ä¸ºå¾€å¾€ä¼˜åŒ–biasï¼Œä¼šè®©viaranceå˜å·®ï¼Œåä¹‹äº¦ç„¶ã€‚

ä½†åœ¨deep learningä¸­ï¼Œä¸Šè¿°æ–¹æ³•åœ¨æé«˜ä¸€ä¸ªæŒ‡æ ‡çš„æ—¶å€™ï¼Œå¾€å¾€å¹¶ä¸ä¼šå¤ªå½±å“å¦ä¸€ä¸ªæŒ‡æ ‡ã€‚æ‰€ä»¥ï¼Œå¦‚æœä½ å·²ç»ä½¿ç”¨äº†æ­£åˆ™åŒ–ï¼Œé‚£ä¹ˆï¼Œä½¿ç”¨æ›´å¤§çš„æ¨¡å‹å‡ ä¹ä¸ä¼šæœ‰ä»€ä¹ˆè´Ÿé¢å½±å“ï¼Œé€ æˆçš„å½±å“åªæ˜¯è®¡ç®—é‡çš„å¢åŠ è€Œå·²ï¼ˆTraining a bigger network almost never hurts. And the main cost of training a neural network that's too big is just computational time, so long as you're regularizing. ï¼‰

![basic-recipe-for-machine-learning.png](https://raw.githubusercontent.com/daiwk/dl.ai/master/c2/imgs/basic-recipe-for-machine-learning.png)

# regularizing your neural network

## regularization

åœ¨lrä¸­ï¼Œä¸€èˆ¬åªå¯¹wåŠ æ­£åˆ™ï¼Œå› ä¸ºbæ˜¯ä¸€ä¸ªå®æ•°ï¼Œè€Œwçš„ç»´åº¦è¾ƒé«˜$n_x$ï¼Œæ‰€ä»¥bçš„å½±å“å¯ä»¥å¿½ç•¥ä¸è®¡ã€‚

![regression-lr.png](https://raw.githubusercontent.com/daiwk/dl.ai/master/c2/imgs/regression-lr.png)

åœ¨ç¥ç»ç½‘ç»œçš„è®­ç»ƒä¸­ï¼Œl1ç”¨å¤„ä¸å¤§ï¼Œl2ç”¨å¾—å¾ˆå¹¿æ³›ã€‚å…¶ä¸­çš„$\lambda$æ˜¯æ­£åˆ™åŒ–å‚æ•°ï¼Œä¸€èˆ¬é€šè¿‡éªŒè¯é›†æˆ–è€…cross-validationæ¥è®¾ç½®ã€‚

ç¥ç»ç½‘ç»œçš„l2é‡Œï¼ŒèŒƒæ•°æ˜¯F-èŒƒæ•°**çš„å¹³æ–¹**ï¼ˆ**F-èŒƒæ•°=2-èŒƒæ•°=çŸ©é˜µä¸­æ‰€æœ‰å…ƒç´ çš„å¹³æ–¹å’Œå†å¼€å¹³æ–¹**ï¼‰ï¼Œä¹Ÿå«åšweight decay(å› ä¸ºåœ¨å¯¹wè¿›è¡Œæ¢¯åº¦ä¸‹é™æ—¶ï¼Œç›¸å½“äºç»™wä¹˜ä¸Šäº†ä¸€ä¸ªå°äº1çš„å› å­ï¼š$(1-\alpha \frac{\lambda }{m})$)ã€‚

![neural-network-lr.png](https://raw.githubusercontent.com/daiwk/dl.ai/master/c2/imgs/neural-network-lr.png)

## why regularization reduces overfitting?

ç›´è§‚åœ°çœ‹ï¼Œå½“$\lambda$å¾ˆå¤§æ—¶ï¼Œæœ€å°åŒ–lossï¼Œä¼šä½¿å¾—wè¶‹å‘0ï¼Œè¿™æ ·ï¼Œç›¸å½“äºå¾ˆå¤šç¥ç»å…ƒæ— å½¢åœ°è¢«å¹²æ‰äº†ï¼Œæ¨¡å‹å˜ç®€å•äº†ã€‚

![how-does-regularization-reduces-overfitting.png](https://raw.githubusercontent.com/daiwk/dl.ai/master/c2/imgs/how-does-regularization-reduces-overfitting.png)

æ¥ä¸‹æ¥ï¼Œwæ¥è¿‘0ï¼Œæ‰€ä»¥zä¹Ÿæ¯”è¾ƒå°ï¼Œè€Œå¦‚æœæ¿€æ´»å‡½æ•°æ˜¯tanh(z)ï¼Œé‚£ä¹ˆï¼Œtanh(z)çš„è¿™ä¸ªåŒºåŸŸé‡Œæ˜¯æ¥è¿‘çº¿æ€§çš„ï¼Œæ‰€ä»¥æ¨¡å‹å°±æ›´åƒä¸€ä¸ªæ¯”è¾ƒå¤§çš„çº¿æ€§å›å½’ã€‚

å¦å¤–ï¼Œå› ä¸ºåŠ äº†æ­£åˆ™åŒ–é¡¹ï¼Œæ‰€ä»¥åŸæ¥çš„Jå¯èƒ½ä¸ä¼šåœ¨æ¯ä¸ªelevations(è°ƒå¹…æ•°é‡ï¼Ÿï¼Ÿçœ‹ç€åˆåƒiteration..)éƒ½å•è°ƒé€’å‡ï¼Œè¦çœ‹æ–°çš„J

![how-does-regularization-reduces-overfitting-2.png](https://raw.githubusercontent.com/daiwk/dl.ai/master/c2/imgs/how-does-regularization-reduces-overfitting-2.png)

## dropout regularization

ç®€å•ç†è§£ï¼Œdropoutå°±æ˜¯å¯¹æ¯ä¸€å±‚è®¾ç½®ä¸€ä¸ªdropout rateï¼Œåœ¨è®­ç»ƒæ—¶ï¼Œé’ˆå¯¹æ¯ä¸€æ¡è®­ç»ƒæ ·æœ¬ï¼Œä»¥è¿™ä¸ªæ¯”ä¾‹æŠŠæŸäº›ç¥ç»å…ƒåŠä¸å…¶è¿æ¥çš„æƒé‡ç›´æ¥å»æ‰ã€‚

![dropout-regularization-introduction.png](https://raw.githubusercontent.com/daiwk/dl.ai/master/c2/imgs/dropout-regularization-introduction.png)

dropoutçš„å®ç°æ–¹å¼ï¼š
keep_prob=0.8ï¼Œè¡¨ç¤ºdropout_rate=0.2
ç¬¬ä¸‰è¡Œçš„a3/=keep_probï¼Œå°±æ˜¯inverted dropoutçš„ç²¾é«“ï¼Œè¿™æ˜¯ä¸ºäº†è®©a3çš„æœŸæœ›å’Œæ²¡æœ‰åšdropoutä¿æŒä¸€è‡´ã€‚åŠ äº†è¿™å¥è¯ï¼Œåœ¨testçš„æ—¶å€™å°±æ›´åŠ å®¹æ˜“äº†ï¼Œåœ¨æ—©æœŸçš„å®ç°ä¸­ï¼Œå¯èƒ½æ²¡è¿™å¥ï¼Œtestå°±ä¼šæ¯”è¾ƒå¤æ‚ã€‚

![dropout-regularization-implementation.png](https://raw.githubusercontent.com/daiwk/dl.ai/master/c2/imgs/dropout-regularization-implementation.png)

testé˜¶æ®µï¼Œ**ä¸è¦ä½¿ç”¨dropout**ï¼Œå› ä¸ºæˆ‘ä»¬å¹¶ä¸æƒ³è®©é¢„æµ‹çš„ç»“æœæ˜¯randomçš„ï¼Œæˆ–è€…æ˜¯æœ‰å™ªéŸ³çš„ã€‚å¦‚æœç”¨äº†inverted dropoutï¼Œ**åœ¨testçš„æ—¶å€™ï¼Œå°±ä¸éœ€è¦åšè¿™ä¸ª/=keep_probçš„æ“ä½œäº†ã€‚**

![dropout-regularization-test-implementation.png](https://raw.githubusercontent.com/daiwk/dl.ai/master/c2/imgs/dropout-regularization-test-implementation.png)

## understanding dropout

dropoutç›¸å½“äºï¼Œæ¯ä¸ªç¥ç»å…ƒä¸åªä»…ä¾èµ–æŸä¸€ä¸ªè¾“å…¥ï¼Œæ‰€ä»¥ä¼šå€¾å‘äºå°†ä¸å„è¾“å…¥ç¥ç»å…ƒçš„è¿æ¥æƒé‡çš„å€¼åˆ†æ•£å¼€æ¥ã€‚==>å¯ä»¥shrink squared norm of the weightï¼Œç±»ä¼¼äºL2çš„æ•ˆæœã€‚

ä¸€èˆ¬ä¸ºä¸åŒçš„å±‚è®¾ç½®ä¸åŒçš„keep_probï¼Œinputå±‚ä¸€èˆ¬æ¥è¿‘1.0ï¼Œå¦‚æœæŸä¸¤å±‚é—´çš„çŸ©é˜µæ¯”è¾ƒå¤§ï¼Œå¯ä»¥è®¾å°ä¸€ç‚¹çš„keep_probï¼Œä¾‹å¦‚å›¾ä¸­çš„0.7, 0.5, 0.7ã€‚

ä¸€èˆ¬åªæœ‰åœ¨å¯èƒ½overfittingçš„æ—¶å€™æ‰ç”¨dropoutï¼Œä¾‹å¦‚cvä¸­ï¼Œå¾€å¾€è¾“å…¥çš„åƒç´ å¾ˆå¤šï¼Œä½†æ•°æ®é‡æ²¡é‚£ä¹ˆå¤§ï¼Œæ‰€ä»¥å¸¸ç”¨dropoutæ¥é¿å…è¿‡æ‹Ÿåˆã€‚

downside of dropoutï¼šcost function J is no longer well defined...æ‰€ä»¥æ¯ä¸€è½®è¿­ä»£åJçš„æ›²çº¿å¹¶ä¸ä¸€å®šæ˜¯å•è°ƒé€’å‡çš„ï¼Œä¸€èˆ¬çš„åšæ³•æ˜¯ï¼Œå…ˆå…³é—­dropoutï¼ˆkeep_prob=1ï¼‰ï¼Œç„¶åè°ƒæ•´æ¨¡å‹åˆ°Jæ›²çº¿æ˜¯é€’å‡çš„ï¼Œå†å¼€å¯dropoutï¼Œçœ‹çœ‹dropoutæœ‰æ²¡æœ‰å¼•å…¥bugã€‚ã€‚

![dropout-regularization-understanding-dropout.png](https://raw.githubusercontent.com/daiwk/dl.ai/master/c2/imgs/dropout-regularization-understanding-dropout.png)

## other regularization methods

### data augmentation

ä¾‹å¦‚å°†å›¾ç‰‡æ°´å¹³æ—‹è½¬ï¼Œå‚ç›´æ—‹è½¬ï¼Œå‰ªåˆ‡ï¼Œç¼©æ”¾ï¼Œæ—‹è½¬ä¸€å®šè§’åº¦ï¼Œå¢åŠ å™ªéŸ³ï¼Œå¯ä»¥å¿«é€Ÿåœ°æ‰©å……è®­ç»ƒæ ·æœ¬ã€‚

![other-regularization-data-augmentation.png](https://raw.githubusercontent.com/daiwk/dl.ai/master/c2/imgs/other-regularization-data-augmentation.png)

### early stopping

åœ¨å›¾ä¸­ä¸€æ–¹é¢ç”»å‡ºtraining setçš„erroræˆ–è€…cost function J(ä¸€èˆ¬ä¼šä¸€ç›´ä¸‹é™)ï¼Œå¦ä¸€æ–¹é¢ï¼Œç”»å‡ºdev setçš„errorï¼ˆä¸€èˆ¬ä¼šä¸‹é™åˆ°ä¸€ä¸ªä½è°·ï¼Œç„¶åä¸Šå‡ï¼‰ã€‚

early stoppingå°±æ˜¯åœ¨dev setåˆ°è¾¾ä½è°·æ—¶ï¼Œåœæ­¢è®­ç»ƒã€‚

å› ä¸ºåœ¨å¼€å§‹è¿­ä»£æ—¶ï¼Œä¸€èˆ¬å‚æ•°wä¼šæ¥è¿‘0ï¼ˆä¸€èˆ¬éšæœºåˆå§‹åŒ–æ—¶ä¼šåˆå§‹åŒ–æˆæ¯”è¾ƒå°çš„å€¼ï¼‰ï¼Œè€Œè®­ç»ƒè½®æ•°å¤ªå¤šæ—¶ï¼Œå¯èƒ½wå°±ä¼šå˜å¾—å¾ˆå¤§ï¼Œæ‰€ä»¥early stoppingæ—¶ï¼Œå¯èƒ½$||w||_F^2$æ­£å¥½æ˜¯ä¸­é—´å¤§å°ã€‚

orthogonalization:

+ optimize cost function J(ä¾‹å¦‚sgdï¼Œmomentumç­‰)
+ not overfittingï¼ˆä¾‹å¦‚regularizationç­‰ï¼‰

ä¸Šé¢äºŒè€…å…¶å®æ˜¯ç›¸äº’æ­£äº¤çš„ï¼Œä¹Ÿå°±æ˜¯å¯ä»¥ç›¸äº’ç‹¬ç«‹åœ°ä¼˜åŒ–ï¼Œä½†dropoutçš„downsideå°±åœ¨äºå®ƒå°†äºŒè€…ç»“åˆåœ¨ä¸€èµ·äº†ï¼Œå› æ­¤æ— æ³•ç‹¬ç«‹åœ°ä¼˜åŒ–ä¸¤ä¸ªtaskã€‚

å¦‚æœä¸ä½¿ç”¨dropoutï¼Œä¸€èˆ¬ä½¿ç”¨L2æ­£åˆ™ï¼Œè¿™æ ·å°±èƒ½è®­ç»ƒå¾ˆä¹…ï¼Œå¹¶ä¸”ä¼šä½¿å¾—è¶…å‚çš„æœç´¢ç©ºé—´æ›´åŠ å®¹æ˜“åˆ†è§£ï¼Œä¹Ÿæ›´å®¹æ˜“æœç´¢ã€‚ä½†L2æ­£åˆ™çš„downsideæ˜¯éœ€è¦searchéå¸¸å¤šçš„$\lambda$ã€‚è€Œearly stoppingåªéœ€è¦è·‘ä¸€æ¬¡æ¢¯åº¦ä¸‹é™ï¼Œå°±èƒ½å¤Ÿè¯•ésmall/large/midçš„wï¼Œä¸ç”¨è¯•é‚£ä¹ˆå¤šçš„$\lambda$ã€‚

![other-regularization-early-stopping.png](https://raw.githubusercontent.com/daiwk/dl.ai/master/c2/imgs/other-regularization-early-stopping.png)

# setting up your optimization problem

## normalizing inputs

åŠ é€Ÿè®­ç»ƒçš„ä¸€ç§æ–¹æ³•å°±æ˜¯normalize inputsï¼Œåˆ†ä¸ºä¸¤æ­¥ï¼š

1. substract out(zero out) mean:

$$\\\mu =\frac{1}{m} \sum _{i=1}^mx^{(i)}
\\x=x-\mu
$$

å˜æˆ0å‡å€¼çš„åˆ†å¸ƒ

2. normalize the varianceï¼š

å¦‚ç¬¬äºŒå¼ å›¾ï¼Œx1å’Œx2çš„æ–¹å·®å·®å¾ˆè¿œï¼Œæ‰€ä»¥éœ€è¦

$$
\\\sigma ^2=\frac{1}{m}\sum _{i=1}^mx^{(i)}**2
\\x/=\sigma ^2
$$

å…¶ä¸­çš„**è¡¨ç¤ºï¼Œelement-wise squaringã€‚

æ³¨æ„ï¼Œå¯¹è®­ç»ƒé›†åšäº†ä¸Šè¿°normalizeä¹‹åï¼Œå˜ä¸º0å‡å€¼ï¼Œ1æ–¹å·®ï¼Œ**å¯¹test setä¹Ÿè¦ç”¨ç›¸åŒçš„$\mu$å’Œ$\sigma$ã€‚**

![normalizing-training-sets.png](https://raw.githubusercontent.com/daiwk/dl.ai/master/c2/imgs/normalizing-training-sets.png)

å‡è®¾wæ˜¯ä¸€ç»´çš„ï¼Œå¦‚æœæ²¡æœ‰normalizeï¼Œé‚£ä¹ˆå¦‚å·¦å›¾ï¼Œå¯èƒ½x1å’Œx2çš„èŒƒå›´å·®å¾ˆè¿œï¼Œè€Œw1å’Œw2ï¼ˆå³wå’Œbï¼‰å¯èƒ½ä¹Ÿå·®å¾ˆè¿œï¼Œå°±éœ€è¦ç”¨éå¸¸å°çš„learning rateï¼›è€Œå¦‚æœnormalizeäº†ï¼Œç­‰é«˜çº¿ï¼ˆä¸Šå›¾æ²¿ç€Jè½´ä¿¯è§†å¾—åˆ°ä¸‹å›¾ï¼‰å°±ç›¸å¯¹å¯¹ç§°äº†ï¼Œå¯ä»¥é‡‡ç”¨æ¯”è¾ƒå¤§çš„learning rateï¼Œå¾ˆå¿«åœ°åˆ°è¾¾Jçš„æå°ç‚¹ã€‚å½“ç„¶ï¼Œç°å®ä¸­wæ˜¯å¤šç»´çš„ï¼Œä½†ä¹Ÿç±»ä¼¼ã€‚

![why-normalize-inputs.png](https://raw.githubusercontent.com/daiwk/dl.ai/master/c2/imgs/why-normalize-inputs.png)

å½“ç„¶ï¼Œå¦‚æœx1,x2,x3èŒƒå›´ä¸ä¼šå·®å¾ˆè¿œï¼Œä¹Ÿä¸ä¸€å®šè¦ç”¨normalizeï¼Œä½†ç”¨äº†ä»å¯èƒ½ä¼šåŠ é€Ÿã€‚

## vanishing/exploding gradients

## weight initialization for deep networks

## numerical approximation of gradients

## gradient checking

## gradient checking implementation notes



