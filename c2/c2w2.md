contents

  * [mini-batch gradient descent](#mini-batch-gradient-descent)
  * [understanding mini-batch gradient descent](#understanding-mini-batch-gradient-descent)
  * [exponentially weighted averages](#exponentially-weighted-averages)
  * [understanding exponentially weighted averages](#understanding-exponentially-weighted-averages)
  * [bias correction in exponentially weighted averages](#bias-correction-in-exponentially-weighted-averages)
  * [gradient descent with momentum](#gradient-descent-with-momentum)
  * [RMSprop](#rmsprop)
  * [Adam optimization algorithm](#adam-optimization-algorithm)
  * [learning rate decay](#learning-rate-decay)
  * [the problem of local optima](#the-problem-of-local-optima)

## mini-batch gradient descent

## understanding mini-batch gradient descent

## exponentially weighted averages

## understanding exponentially weighted averages

## bias correction in exponentially weighted averages

## gradient descent with momentum

## RMSprop

## Adam optimization algorithm

## learning rate decay

## the problem of local optima


