contents

  * [deep L-layer neural network](#deep-l-layer-neural-network)
  * [forward propagation in a deep network](#forward-propagation-in-a-deep-network)
  * [getting your matrix dimensions right](#getting-your-matrix-dimensions-right)
  * [why deep representations?](#why-deep-representations)
  * [building blocks of deep neural networks](#building-blocks-of-deep-neural-networks)
  * [forward and backward propagation](#forward-and-backward-propagation)
  * [parameters vs hyperparameters](#parameters-vs-hyperparameters)
  * [what does this have to do with the brain?](#what-does-this-have-to-do-with-the-brain)

## deep L-layer neural network

![deep-neural-network-notation.png](https://raw.githubusercontent.com/daiwk/dl.ai/master/c1/imgs/deep-neural-network-notation.png)

## forward propagation in a deep network

$z^{[l](i)}$表示第l层的第i个训练样本(列向量)，$Z^{[l]}$表示将第l层的这$m$个训练样本全部水平地放在一起形成的矩阵。以此vectorization的方法，可以避免从1->$m$的这个for-loop。

另外，从第1层到第4层这个for-loop是无法避免的。

![forward-propagation-in-a-deep-network.png](https://raw.githubusercontent.com/daiwk/dl.ai/master/c1/imgs/forward-propagation-in-a-deep-network.png)

## getting your matrix dimensions right

![w-and-b-dimension.png](https://raw.githubusercontent.com/daiwk/dl.ai/master/c1/imgs/w-and-b-dimension.png)

vectorized之后，$b^{[l]}$仍然是$(n^{[l]},1)$维，只是因为broadcasting，才复制m遍，变成了$(n^{[l]},m)$维。

![vectorized-implementation.png](https://raw.githubusercontent.com/daiwk/dl.ai/master/c1/imgs/vectorized-implementation.png)

## why deep representations?

## building blocks of deep neural networks

## forward and backward propagation

## parameters vs hyperparameters

## what does this have to do with the brain?


