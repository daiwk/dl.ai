<p>contents</p>
<ul>
<li><a href="#logistic-regression-as-a-neural-network">logistic regression as a neural network</a></li>
<li><a href="#binary-classification">binary classification</a></li>
<li><a href="#logistic-regression">logistic regression</a></li>
<li><a href="#logistic-regression-cost-function">logistic regression cost function</a></li>
<li><a href="#gradient-descent">gradient descent</a></li>
<li><a href="#derivatives">derivatives</a></li>
<li><a href="#more-derivative-examples">more derivative examples</a></li>
<li><a href="#computation-graph">computation graph</a></li>
<li><a href="#derivatives-with-a-computation-graph">derivatives with a computation graph</a></li>
<li><a href="#logistic-regression-gradient-descent">logistic regression gradient descent</a></li>
<li><a href="#gradient-descent-on-m-examples">gradient descent on m examples</a></li>
<li><a href="#python--vectorization">python &amp; vectorization</a></li>
<li><a href="#vectorization">vectorization</a></li>
<li><a href="#more-examples-of-vectorization">more examples of vectorization</a></li>
<li><a href="#vectorizing-logistic-regression">vectorizing logistic regression</a></li>
<li><a href="#vectorizing-logistic-regressions-gradient-output">vectorizing logistic regression's gradient output</a></li>
<li><a href="#broadcasting-in-python">broadcasting in python</a></li>
<li><a href="#a-note-on-pythonnumpy-vectors">a note on python/numpy vectors</a></li>
<li><a href="#quick-tour-of-jupyteripython-notebooks">quick tour of jupyter/ipython notebooks</a></li>
<li><a href="#explanation-of-logistic-regression-cost-function">explanation of logistic regression cost function</a></li>
</ul>
<h1 id="logistic-regression-as-a-neural-network">logistic regression as a neural network</h1>
<h2 id="binary-classification">binary classification</h2>
<p>维度为(64, 64, 3)的图片 ===&gt; img vector: x=维度为(64*64*3=12288, 1)的列向量。(<span class="math inline"><em>n</em><sub><em>x</em></sub> = 12288</span>)</p>
<p><br /><span class="math display">$$ (x,y), x \in R^{n_x}, y \in \\{0,1\\} $$</span><br /> <span class="math inline"><em>m</em> = <em>m</em><sub><em>t</em><em>r</em><em>a</em><em>i</em><em>n</em></sub></span>个训练样本：<span class="math inline">(<em>x</em><sup>(1)</sup>, <em>y</em><sup>(1)</sup>),...,(<em>x</em><sup>(<em>m</em>)</sup>, <em>y</em><sup>(<em>m</em>)</sup>), <em>m</em><sub><em>t</em><em>e</em><em>s</em><em>t</em></sub></span>个测试样本。</p>
<p><span class="math inline"><em>X</em></span>表示一个<span class="math inline">$n_x\*m$</span>的训练样本矩阵,在python里就是<code>X.shape=(n_x,m)</code> <span class="math inline"><em>Y</em></span>表示一个<span class="math inline">$1\*m$</span>的向量,在python里是<code>Y.shape=(1,m)</code></p>
<h2 id="logistic-regression">logistic regression</h2>
<p>given <span class="math inline"><em>x</em></span>, want <span class="math inline">$\hat y=P(y=1|x), x \in R^{n_x}$</span></p>
<p>params: <span class="math inline"><em>w</em> ∈ <em>R</em><sup><em>n</em><sub><em>x</em></sub></sup>, <em>b</em> ∈ <em>R</em></span></p>
<p>output: <span class="math inline">$\hat y= \sigma(w^Tx+b), \sigma(z)=\frac{1}{1+e^(-z)}$</span></p>
<h2 id="logistic-regression-cost-function">logistic regression cost function</h2>
<p><span class="math inline">(<em>x</em><sup>(</sup><em>i</em>),<em>y</em><sup>(</sup><em>i</em>))</span> 表示第i个样本。</p>
<p><strong>Loss(error) function只针对一条训练样本：</strong></p>
<ul>
<li>square error的loss function: <br /><span class="math display">$$L(\hat y, y)=1/2*(\hat y - y)^2$$</span><br /></li>
<li>logistic regression的loss function: <br /><span class="math display">$$L(\hat y, y)=-(ylog\hat y+(1-y)log(1-\hat y))$$</span><br /></li>
</ul>
<p>if $y=1, L(y, y) = -logy $, want <span class="math inline">$log\hat y$</span> large, want <span class="math inline">$\hat y$</span> large</p>
<p>if $y=0, L(y, y) = -log(1-y) $, want <span class="math inline">$\hat y$</span> small</p>
<p><strong>Cost function针对全体训练样本:</strong> <br /><span class="math display">$$J(w,b)=1/m\sum ^m_{i=1}L(\hat y^{(i)}, y^{(i)})=-1/m\sum^m_{i=1}[y^{(i)}log\hat y^{(i)}+(1-y^{(i)})log(1-\hat y^{(i)})]$$</span><br /></p>
<h2 id="gradient-descent">gradient descent</h2>
<p>lr的<span class="math inline"><em>J</em>(<em>w</em>, <em>b</em>)</span>是一个凸函数，所以有全局最优。 因为有全局最优，所以lr的初始化一般是0，不用随机。梯度下降：不断重复$ w=w-<span class="math inline"><em>直</em><em>到</em><em>收</em><em>敛</em>。<em>后</em><em>续</em>，<em>用</em></span>dw<span class="math inline"><em>来</em><em>指</em><em>代</em></span>$。梯度下降的公式： <br /><span class="math display"><em>w</em> = <em>w</em> − <em>α</em><em>d</em><em>w</em></span><br /> <br /><span class="math display"><em>b</em> = <em>b</em> − <em>α</em><em>d</em><em>b</em></span><br /></p>
<h2 id="derivatives">derivatives</h2>
<p>derivative = slope，就是<span class="math inline"><em>d</em><em>y</em>/<em>d</em><em>x</em> = <em>Δ</em><em>y</em>/<em>Δ</em><em>x</em></span></p>
<h2 id="more-derivative-examples">more derivative examples</h2>
<p><span class="math inline">$f(a)=log_e(a)=ln(a), df(a)/da=\frac{1}{a}$</span></p>
<h2 id="computation-graph">computation graph</h2>
<p>正向计算图算出输出，算每个参数的梯度就反向算。</p>
<div class="figure">
<img src="https://raw.githubusercontent.com/daiwk/dl.ai/master/c1/imgs/computation%20graph.png" alt="computation graph" />
<p class="caption">computation graph</p>
</div>
<h2 id="derivatives-with-a-computation-graph">derivatives with a computation graph</h2>
<p><br /><span class="math display"><em>J</em> = 3<em>v</em>, <em>v</em> = <em>a</em> + <em>u</em>, <em>u</em> = <em>b</em><em>c</em></span><br /> <br /><span class="math display">$$\frac {dJ}{dv}=3, \frac{dv}{da}=1$$</span><br /> <br /><span class="math display">$$so, \frac {dJ}{da}=\frac {dJ}{dv}\frac {dv}{da}=3\*1=3$$</span><br /> <br /><span class="math display">$$if\ b=2,then\ \frac{dJ}{dc}=\frac{dJ}{dv}\frac{dv}{du}\frac{du}{dc}=3\*1\*b=3\*1\*2=6$$</span><br /> <strong>写代码时，将<span class="math inline">$\frac{dFinalOutputVar}{dvar}$</span>记为<span class="math inline"><em>d</em><em>v</em><em>a</em><em>r</em></span>(最后输出对这个变量的偏导)</strong></p>
<div class="figure">
<img src="https://raw.githubusercontent.com/daiwk/dl.ai/master/c1/imgs/computation%20graph-derivatives.png" alt="derivatives in computation graph" />
<p class="caption">derivatives in computation graph</p>
</div>
<h2 id="logistic-regression-gradient-descent">logistic regression gradient descent</h2>
<p><br /><span class="math display"><em>z</em> = <em>w</em><sup><em>T</em></sup><em>x</em> + <em>b</em></span><br /> <br /><span class="math display">$$\hat y=a=\sigma (z)$$</span><br /> <br /><span class="math display"><em>L</em>(<em>a</em>, <em>y</em>)= − (<em>y</em><em>l</em><em>o</em><em>g</em>(<em>a</em>)+(1 − <em>y</em>)<em>l</em><em>o</em><em>g</em>(1 − <em>a</em>))</span><br /></p>
<div class="figure">
<img src="https://raw.githubusercontent.com/daiwk/dl.ai/master/c1/imgs/computation%20graph-derivatives-lr.png" alt="derivatives in computation graph in lr" />
<p class="caption">derivatives in computation graph in lr</p>
</div>
<h2 id="gradient-descent-on-m-examples">gradient descent on m examples</h2>
<p>首先，根据J的公式，可以知道dJ/dw1其实就是对每个样本的dw1求和，然后/m。</p>
<div class="figure">
<img src="https://raw.githubusercontent.com/daiwk/dl.ai/master/c1/imgs/gradient_descent_lr_m_examples_djdw.png" alt="gradient_descent_lr_m_examples_djdw" />
<p class="caption">gradient_descent_lr_m_examples_djdw</p>
</div>
<p>每一次迭代，遍历m个样本，算出J/dw1/dw2/db，然后用这些梯度去更新一次w1/w2/b。</p>
<div class="figure">
<img src="https://raw.githubusercontent.com/daiwk/dl.ai/master/c1/imgs/gradient_descent_lr_m_examples.png" alt="gradient_descent_lr_m_examples" />
<p class="caption">gradient_descent_lr_m_examples</p>
</div>
<p>但这的for loop太多了。。所以我们需要vectorization!</p>
<h1 id="python-vectorization">python &amp; vectorization</h1>
<h2 id="vectorization">vectorization</h2>
<div class="figure">
<img src="https://raw.githubusercontent.com/daiwk/dl.ai/master/c1/imgs/vectorization-1.png" alt="gradient_descent_lr_m_examples" />
<p class="caption">gradient_descent_lr_m_examples</p>
</div>
<p>对于两个100w维的向量进行点乘，vectorization(1.5ms) 比for loop(470ms+)快</p>
<h2 id="more-examples-of-vectorization">more examples of vectorization</h2>
<div class="figure">
<img src="https://raw.githubusercontent.com/daiwk/dl.ai/master/c1/imgs/vectorization-2.png" alt="gradient_descent_lr_m_examples" />
<p class="caption">gradient_descent_lr_m_examples</p>
</div>
<div class="figure">
<img src="https://raw.githubusercontent.com/daiwk/dl.ai/master/c1/imgs/vectorization-3.png" alt="gradient_descent_lr_m_examples" />
<p class="caption">gradient_descent_lr_m_examples</p>
</div>
<p><img src="https://raw.githubusercontent.com/daiwk/dl.ai/master/c1/imgs/vectorization-lr.png" alt="gradient_descent_lr_m_examples" /> 如上图，将<span class="math inline"><em>n</em><sub><em>x</em></sub></span>维的dw变为一个np.array即可干掉内层的for loop。</p>
<h2 id="vectorizing-logistic-regression">vectorizing logistic regression</h2>
<p>可见，整个求<span class="math inline"><em>Z</em></span>的过程可以变成一句话，而求A时，需要封装一个基于numpy的sigmoid函数。</p>
<div class="figure">
<img src="https://raw.githubusercontent.com/daiwk/dl.ai/master/c1/imgs/vectorization-lr-2.png" alt="gradient_descent_lr_m_examples" />
<p class="caption">gradient_descent_lr_m_examples</p>
</div>
<h2 id="vectorizing-logistic-regressions-gradient-output">vectorizing logistic regression's gradient output</h2>
<div class="figure">
<img src="https://raw.githubusercontent.com/daiwk/dl.ai/master/c1/imgs/vectorization-lr-3.png" alt="gradient_descent_lr_m_examples" />
<p class="caption">gradient_descent_lr_m_examples</p>
</div>
<div class="figure">
<img src="https://raw.githubusercontent.com/daiwk/dl.ai/master/c1/imgs/vectorization-lr-4.png" alt="gradient_descent_lr_m_examples" />
<p class="caption">gradient_descent_lr_m_examples</p>
</div>
<h2 id="broadcasting-in-python">broadcasting in python</h2>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python">A <span class="op">=</span> ndarray([[<span class="dv">1</span>,<span class="dv">2</span>,<span class="dv">3</span>,<span class="dv">4</span>],[<span class="dv">2</span>,<span class="dv">3</span>,<span class="dv">4</span>,<span class="dv">5</span>],[<span class="dv">3</span>,<span class="dv">4</span>,<span class="dv">5</span>,<span class="dv">6</span>]]) <span class="co"># 3*4</span>
calc <span class="op">=</span> A.<span class="bu">sum</span>(axis<span class="op">=</span><span class="dv">0</span>) <span class="co"># A的每列求和,得到1*4</span>
calc2 <span class="op">=</span> A.<span class="bu">sum</span>(axis<span class="op">=</span><span class="dv">1</span>) <span class="co"># A的每行求和,得到3*1</span>
A<span class="op">/</span>calc.reshape(<span class="dv">1</span>,<span class="dv">4</span>) <span class="co">#得到一个3*4的矩阵，就是broadcasting。其实等价于A/calc，但为了保险，可以调用reshape(1,4)来确保无误</span></code></pre></div>
<p>小结：</p>
<div class="figure">
<img src="https://raw.githubusercontent.com/daiwk/dl.ai/master/c1/imgs/broadcasting.png" alt="broadcasting" />
<p class="caption">broadcasting</p>
</div>
<h2 id="a-note-on-pythonnumpy-vectors">a note on python/numpy vectors</h2>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python">a<span class="op">=</span>np.random.randn(<span class="dv">5</span>) <span class="co"># a.shape=(5,)是一个vector(rank 1 array)，不是一个矩阵，所以a.T还是(5,)，np.dot(a,a.T)=np.dot(a.T,a)是个1*1的数字</span>

b<span class="op">=</span>np.random.randn(<span class="dv">5</span>,<span class="dv">1</span>) <span class="co"># a.shape=(5,1),  a.T.shape=(1,5), np.dot(a,a.T)是一个5*5的，np.dot(a.T,a)是一个1*1的矩阵（类似array[[0.444]]））</span>

<span class="co">## 可以加一句：</span>
<span class="cf">assert</span>(a.shape <span class="op">==</span> (<span class="dv">5</span>, <span class="dv">1</span>))
<span class="co">## 如果不小心搞了个rank 1 array,也可以手动a.reshape((5,1))=a.reshape(5,1)</span></code></pre></div>
<h2 id="quick-tour-of-jupyteripython-notebooks">quick tour of jupyter/ipython notebooks</h2>
<h2 id="explanation-of-logistic-regression-cost-function">explanation of logistic regression cost function</h2>
<p>单个样本的loss function, log越大，loss越小：</p>
<div class="figure">
<img src="https://raw.githubusercontent.com/daiwk/dl.ai/master/c1/imgs/lr-loss.png" alt="lr-loss" />
<p class="caption">lr-loss</p>
</div>
<p>如果是iid（独立同分布），那么，m个样本的cost function，其实就叫对数似然。对他求极大似然估计，其实就是对m个样本求每个cost function的min:</p>
<div class="figure">
<img src="https://raw.githubusercontent.com/daiwk/dl.ai/master/c1/imgs/lr-cost-m-examples.png" alt="lr-cost-m-examples" />
<p class="caption">lr-cost-m-examples</p>
</div>
<h2 id="programming-assignments">programming assignments</h2>
<p>squeeze</p>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python">np.squeeze(a, axis<span class="op">=</span><span class="va">None</span>)
<span class="co">## 删掉维数是一的部分，axis可以是Int/int数组，表示只去掉指定下标的部分，如果该部分维数不是1，会报错</span>
x <span class="op">=</span> np.array([[[<span class="dv">0</span>], [<span class="dv">1</span>], [<span class="dv">2</span>]]])
x.shape<span class="op">=</span>(<span class="dv">1</span>,<span class="dv">3</span>,<span class="dv">1</span>)
np.squeeze(x)<span class="op">=</span>array([<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">2</span>]) <span class="co"># shape=(3,)</span>
np.squeeze(x, axis<span class="op">=</span>(<span class="dv">2</span>,))<span class="op">=</span>array([[<span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">2</span>]]) <span class="co"># shape=(1,3)</span></code></pre></div>
<p>把一个shape是(a,b,c,d)的array转成一个type是(b<em>c</em>d,a)的array:</p>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python">X_flatten <span class="op">=</span> X.reshape(X.shape[<span class="dv">0</span>], <span class="op">-</span><span class="dv">1</span>).T </code></pre></div>
<p>图片的预处理： + Figure out the dimensions and shapes of the problem (m_train, m_test, num_px, ...) + Reshape the datasets such that each example is now a vector of size (num_px * num_px * 3, 1) + &quot;Standardize&quot; the data: 对图片而言，所有元素除以255就可以了</p>
<p>contents</p>
<ul>
<li><a href="#neural-networks-overview">neural networks overview</a></li>
<li><a href="#neural-network-representation">neural network representation</a></li>
<li><a href="#computing-a-neural-networks-output">computing a neural network's output</a></li>
<li><a href="#vectorizing-across-multiple-examples">vectorizing across multiple examples</a></li>
<li><a href="#explanation-for-vectorized-implementation">explanation for vectorized implementation</a></li>
<li><a href="#activation-functions">activation functions</a></li>
<li><a href="#why-do-you-need-non-linear-activation-functions">why do you need non-linear activation functions</a></li>
<li><a href="#derivatives-of-activation-functions">derivatives of activation functions</a></li>
<li><a href="#gradient-descent-for-neural-networks">gradient descent for neural networks</a></li>
<li><a href="#backpropagation-intuition">backpropagation intuition</a></li>
<li><a href="#random-initialization">random initialization</a></li>
</ul>
<h2 id="neural-networks-overview">neural networks overview</h2>
<p>其中，每个神经元完成了<span class="math inline"><em>z</em> = <em>w</em><sup><em>T</em></sup><em>x</em> + <em>b</em></span>以及<span class="math inline"><em>a</em> = <em>σ</em>(<em>z</em>)</span>两个操作(<span class="math inline"><em>a</em></span>表示activation)，每一层的数据用上标[i]表示。</p>
<div class="figure">
<img src="https://raw.githubusercontent.com/daiwk/dl.ai/master/c1/imgs/neural-networks-overview.png" alt="neural-networks-overview" />
<p class="caption">neural-networks-overview</p>
</div>
<h2 id="neural-network-representation">neural network representation</h2>
<p>图示是一个2层nn（inputlayer不算在内，有hidden和output两层）。</p>
<p>如果输入的x有3维，在lr中，<span class="math inline"><em>s</em><em>h</em><em>a</em><em>p</em><em>e</em>(<em>w</em>)=(1, 3)</span>，<span class="math inline"><em>s</em><em>h</em><em>a</em><em>p</em><em>e</em>(<em>b</em>)=(1, 1)</span>。</p>
<p>而在nn中，<span class="math inline"><em>s</em><em>h</em><em>a</em><em>p</em><em>e</em>(<em>w</em><sup>[1]</sup>)=(4, 3)</span>因为有4个神经元，输入是3维。同理<span class="math inline"><em>s</em><em>h</em><em>a</em><em>p</em><em>e</em>(<em>b</em><sup>[1]</sup>)=(4, 1)</span>。</p>
<p>而<span class="math inline"><em>s</em><em>h</em><em>a</em><em>p</em><em>e</em>(<em>w</em><sup>[2]</sup>)=(1, 4)</span>，因为只有1个神经元，输入是3维。同理<span class="math inline"><em>s</em><em>h</em><em>a</em><em>p</em><em>e</em>(<em>b</em><sup>[2]</sup>)=(1, 1)</span>。</p>
<div class="figure">
<img src="https://raw.githubusercontent.com/daiwk/dl.ai/master/c1/imgs/neural-network-representation.png" alt="neural-network-representation" />
<p class="caption">neural-network-representation</p>
</div>
<h2 id="computing-a-neural-networks-output">computing a neural network's output</h2>
<p><img src="https://raw.githubusercontent.com/daiwk/dl.ai/master/c1/imgs/compute-nn-output-1.png" alt="compute-nn-output-1" /> <img src="https://raw.githubusercontent.com/daiwk/dl.ai/master/c1/imgs/compute-nn-output-2.png" alt="compute-nn-output-1" /></p>
<h2 id="vectorizing-across-multiple-examples">vectorizing across multiple examples</h2>
<div class="figure">
<img src="https://raw.githubusercontent.com/daiwk/dl.ai/master/c1/imgs/vectorizing-across-multiple-examples-for-loop.png" alt="vectorizing-across-multiple-examples-for-loop" />
<p class="caption">vectorizing-across-multiple-examples-for-loop</p>
</div>
<p>矩阵<span class="math inline"><em>X</em></span>纵向是x的维数（行数），横向是training examples的个数（列数）。</p>
<p>矩阵<span class="math inline"><em>Z</em></span>、<span class="math inline"><em>A</em></span>纵向是hidden units的个数（行数），横向是training examples的个数（列数）。</p>
<div class="figure">
<img src="https://raw.githubusercontent.com/daiwk/dl.ai/master/c1/imgs/vectorizing-across-multiple-examples-vectorization.png" alt="vectorizing-across-multiple-examples-vectorization" />
<p class="caption">vectorizing-across-multiple-examples-vectorization</p>
</div>
<h2 id="explanation-for-vectorized-implementation">explanation for vectorized implementation</h2>
<p><img src="https://raw.githubusercontent.com/daiwk/dl.ai/master/c1/imgs/justification-for-vectorized-implementation.png" alt="justification-for-vectorized-implementation" /> <img src="https://raw.githubusercontent.com/daiwk/dl.ai/master/c1/imgs/vectorizing-across-multiple-examples-recap.png" alt="vectorizing-across-multiple-examples-recap" /></p>
<h2 id="activation-functions">activation functions</h2>
<p>一般来说，<span class="math inline"><em>t</em><em>a</em><em>n</em><em>h</em></span>效果比<span class="math inline"><em>s</em><em>i</em><em>g</em><em>m</em><em>o</em><em>i</em><em>d</em></span>好，因为均值是0。但对于outputlayer而言，一般<span class="math inline">$y \in \\{0,1\\}$</span>，所以希望<span class="math inline">$0\le \hat y \le 1$</span>，所以会用<span class="math inline"><em>s</em><em>i</em><em>g</em><em>m</em><em>o</em><em>i</em><em>d</em></span>。</p>
<p><span class="math inline"><em>R</em><em>e</em><em>L</em><em>U</em>(<em>z</em>)=<em>m</em><em>a</em><em>x</em>(0, <em>z</em>)</span>比<span class="math inline"><em>t</em><em>a</em><em>n</em><em>h</em></span>好，因为当<span class="math inline"><em>x</em> ≤ 0</span>时，梯度为0。而<span class="math inline"><em>l</em><em>e</em><em>a</em><em>k</em><em>y</em><em>R</em><em>e</em><em>L</em><em>U</em></span>在<span class="math inline"><em>x</em> ≤ 0</span>时，梯度是接近0，效果会好一点，但在实践中还是<span class="math inline"><em>R</em><em>e</em><em>L</em><em>U</em></span>居多。当<span class="math inline"><em>x</em> &gt; 0</span>时，梯度和<span class="math inline"><em>x</em> ≤ 0</span>差很多，所以训练速度会加快。</p>
<div class="figure">
<img src="https://raw.githubusercontent.com/daiwk/dl.ai/master/c1/imgs/activation-functions.png" alt="activation-functions" />
<p class="caption">activation-functions</p>
</div>
<h2 id="why-do-you-need-non-linear-activation-functions">why do you need non-linear activation functions</h2>
<p>linear activation: 因为<span class="math inline"><em>z</em> = <em>w</em><em>x</em> + <em>b</em></span>，所以激活函数<span class="math inline"><em>g</em>(<em>z</em>)=<em>z</em> = <em>w</em><em>x</em> + <em>b</em></span>就叫linear activation function，也叫identity activatioin function。</p>
<p>不要在hidden layer用linear activation functions，因为多个linear嵌套，实质上还是linear。</p>
<p>例外，当进行回归时，<span class="math inline"><em>y</em> ∈ <em>R</em></span>，可以hidden layer用<span class="math inline"><em>R</em><em>e</em><em>L</em><em>U</em></span>，但output layer用linear activation。</p>
<div class="figure">
<img src="https://raw.githubusercontent.com/daiwk/dl.ai/master/c1/imgs/why-use-non-linear-functions.png" alt="why-use-non-linear-functions.png" />
<p class="caption">why-use-non-linear-functions.png</p>
</div>
<h2 id="derivatives-of-activation-functions">derivatives of activation functions</h2>
<p>sigmoid的导数 <img src="https://raw.githubusercontent.com/daiwk/dl.ai/master/c1/imgs/derivative-of-sigmoid.png" alt="derivative-of-sigmoid.png" /></p>
<p>tanh的导数 <img src="https://raw.githubusercontent.com/daiwk/dl.ai/master/c1/imgs/derivative-of-tanh.png" alt="derivative-of-tanh.png" /></p>
<p>relu和leaky relu的导数(z=0时不可导，但在工程上，直接归入z&gt;0部分) <img src="https://raw.githubusercontent.com/daiwk/dl.ai/master/c1/imgs/derivative-of-tanh.png" alt="derivative-of-tanh.png" /></p>
<h2 id="gradient-descent-for-neural-networks">gradient descent for neural networks</h2>
<p><strong>记住每个W/b的shape!</strong> <img src="https://raw.githubusercontent.com/daiwk/dl.ai/master/c1/imgs/gradient-descent-for-neural-networks.png" alt="gradient-descent-for-neural-networks" /></p>
<p>其中的keepdims=True表示，输出的<span class="math inline"><em>s</em><em>h</em><em>a</em><em>p</em><em>e</em> = (<em>n</em><sup>[2]</sup>, 1)</span>而非<span class="math inline">(<em>n</em><sup>[2]</sup>, )</span> 另外求dz时，两项之前是element-wise product(np.multiply)，其中第二项就是对激活函数求在<span class="math inline"><em>z</em><sup>[1]</sup></span>的导数 <img src="https://raw.githubusercontent.com/daiwk/dl.ai/master/c1/imgs/forward-propagation-and-back-propagation.png" alt="forward-propagation-and-back-propagation" /></p>
<h2 id="backpropagation-intuition">backpropagation intuition</h2>
<p>先回顾一下lr： <img src="https://raw.githubusercontent.com/daiwk/dl.ai/master/c1/imgs/back-propagation-lr.png" alt="back-propagation-lr" /></p>
<p>然后看nn： <img src="https://raw.githubusercontent.com/daiwk/dl.ai/master/c1/imgs/back-propagation-nn.png" alt="back-propagation-nn" /></p>
<p>扩展到m个examples，并进行vectorize： <img src="https://raw.githubusercontent.com/daiwk/dl.ai/master/c1/imgs/back-propagation-nn-vectorized.png" alt="back-propagation-nn-vectorized" /> <img src="https://raw.githubusercontent.com/daiwk/dl.ai/master/c1/imgs/back-propagation-nn-vectorized-all.png" alt="back-propagation-nn-vectorized-all" /></p>
<p><strong>问：为何有的有1/m，有的没有。。。。。</strong></p>
<h2 id="random-initialization">random initialization</h2>
<p>lr的训练，参数一般都初始化为0。但nn，如果初始化为0，会发现算出来的<span class="math inline"><em>a</em><sub>1</sub><sup>[1]</sup> = <em>a</em><sub>2</sub><sup>[1]</sup></span>，<span class="math inline"><em>d</em><em>z</em><sub>1</sub><sup>[1]</sup> = <em>d</em><em>z</em><sub>2</sub><sup>[1]</sup></span>，所以相当于每个神经元的influence是一样的(symetric)，所以<span class="math inline"><em>d</em><em>w</em><sup>[1]</sup></span>这个矩阵的每一行都相等。 &quot;Each neuron in the first hidden layer will perform the same computation. So even after multiple iterations of gradient descent each neuron in the layer will be computing the same thing as other neurons.&quot; <img src="https://raw.githubusercontent.com/daiwk/dl.ai/master/c1/imgs/if-initialize-zero.png" alt="if-initialize-zero" /></p>
<p>解决：随机初始化 只要w随机初始化了，b其实影响不大，0就可以了。如果用的是sigmoid/tanh的话，随机初始化时，尽量小，因为如果大的话，激活后会接近两端(无论是+无穷还是-无穷，梯度都接近0)，导致学习过程变得很慢。<strong>对于浅层的网络，0.01就可以了，但如果是更深的网络，可能需要其他系数，这个选择过程，在后面的课程会讲。。。</strong></p>
<div class="figure">
<img src="https://raw.githubusercontent.com/daiwk/dl.ai/master/c1/imgs/random-initialization.png" alt="random-initialization" />
<p class="caption">random-initialization</p>
</div>
<p>contents</p>
<ul>
<li><a href="#deep-l-layer-neural-network">deep L-layer neural network</a></li>
<li><a href="#forward-propagation-in-a-deep-network">forward propagation in a deep network</a></li>
<li><a href="#getting-your-matrix-dimensions-right">getting your matrix dimensions right</a></li>
<li><a href="#why-deep-representations">why deep representations?</a></li>
<li><a href="#building-blocks-of-deep-neural-networks">building blocks of deep neural networks</a></li>
<li><a href="#forward-and-backward-propagation">forward and backward propagation</a></li>
<li><a href="#parameters-vs-hyperparameters">parameters vs hyperparameters</a></li>
<li><a href="#what-does-this-have-to-do-with-the-brain">what does this have to do with the brain?</a></li>
</ul>
<h2 id="deep-l-layer-neural-network">deep L-layer neural network</h2>
<div class="figure">
<img src="https://raw.githubusercontent.com/daiwk/dl.ai/master/c1/imgs/deep-neural-network-notation.png" alt="deep-neural-network-notation.png" />
<p class="caption">deep-neural-network-notation.png</p>
</div>
<h2 id="forward-propagation-in-a-deep-network">forward propagation in a deep network</h2>
<p><span class="math inline">$z^{[l]\(i\)}$</span>表示第l层的第i个训练样本(列向量)，<span class="math inline"><em>Z</em><sup>[<em>l</em>]</sup></span>表示将第l层的这<span class="math inline"><em>m</em></span>个训练样本全部水平地放在一起形成的矩阵。以此vectorization的方法，可以避免从1-&gt;<span class="math inline"><em>m</em></span>的这个for-loop。</p>
<p>另外，从第1层到第4层这个for-loop是无法避免的。</p>
<div class="figure">
<img src="https://raw.githubusercontent.com/daiwk/dl.ai/master/c1/imgs/forward-propagation-in-a-deep-network.png" alt="forward-propagation-in-a-deep-network.png" />
<p class="caption">forward-propagation-in-a-deep-network.png</p>
</div>
<h2 id="getting-your-matrix-dimensions-right">getting your matrix dimensions right</h2>
<div class="figure">
<img src="https://raw.githubusercontent.com/daiwk/dl.ai/master/c1/imgs/w-and-b-dimension.png" alt="w-and-b-dimension.png" />
<p class="caption">w-and-b-dimension.png</p>
</div>
<p>vectorized之后，<span class="math inline"><em>b</em><sup>[<em>l</em>]</sup></span>仍然是<span class="math inline">(<em>n</em><sup>[<em>l</em>]</sup>, 1)</span>维，只是因为broadcasting，才复制m遍，变成了<span class="math inline">(<em>n</em><sup>[<em>l</em>]</sup>, <em>m</em>)</span>维。</p>
<div class="figure">
<img src="https://raw.githubusercontent.com/daiwk/dl.ai/master/c1/imgs/vectorized-implementation.png" alt="vectorized-implementation.png" />
<p class="caption">vectorized-implementation.png</p>
</div>
<h2 id="why-deep-representations">why deep representations?</h2>
<div class="figure">
<img src="https://raw.githubusercontent.com/daiwk/dl.ai/master/c1/imgs/intuition-of-deep-representation.png" alt="intuition-of-deep-representation.png" />
<p class="caption">intuition-of-deep-representation.png</p>
</div>
<p>对于<span class="math inline">(<em>x</em><sub>1</sub>, <em>x</em><sub>2</sub>, ...<em>x</em><sub><em>n</em></sub>)</span>的异或（XOR）操作，如果用树型结构，n个叶子节点，则树深度是<span class="math inline"><em>l</em><em>o</em><em>g</em><sub>2</sub><em>n</em> + 1</span>(深度为k的满二叉树的第i层上有<span class="math inline">2<sup><em>i</em> − 1</sup></span>个结点,总共至多有<span class="math inline">2<sup><em>k</em></sup> − 1<em>个</em><em>结</em><em>点</em></span>)，即只需要<span class="math inline"><em>O</em>(<em>l</em><em>o</em><em>g</em><sub>2</sub><em>n</em>)</span>层的树就能完成。</p>
<p>而如果采用单隐层，则需要<span class="math inline">2<sup>(<em>n</em> − 1)</sup></span>个节点</p>
<div class="figure">
<img src="https://raw.githubusercontent.com/daiwk/dl.ai/master/c1/imgs/circuit-theory-and-deep-learning.png" alt="circuit-theory-and-deep-learning.png" />
<p class="caption">circuit-theory-and-deep-learning.png</p>
</div>
<h2 id="building-blocks-of-deep-neural-networks">building blocks of deep neural networks</h2>
<p>forward时，需要cache <span class="math inline"><em>Z</em><sup>[<em>l</em>]</sup></span>以供backward使用。</p>
<div class="figure">
<img src="https://raw.githubusercontent.com/daiwk/dl.ai/master/c1/imgs/forward-and-backward-functions.png" alt="forward-and-backward-functions.png" />
<p class="caption">forward-and-backward-functions.png</p>
</div>
<p>为了计算backward，其实需要cache的有<span class="math inline"><em>Z</em><sup>[<em>l</em>]</sup></span>、<span class="math inline"><em>W</em><sup>[<em>l</em>]</sup></span>以及<span class="math inline"><em>b</em><sup>[<em>l</em>]</sup></span>：</p>
<div class="figure">
<img src="https://raw.githubusercontent.com/daiwk/dl.ai/master/c1/imgs/forward-backward-functions.png" alt="forward-backward-functions.png" />
<p class="caption">forward-backward-functions.png</p>
</div>
<h2 id="forward-and-backward-propagation">forward and backward propagation</h2>
<p>forward propagation for layer l:</p>
<div class="figure">
<img src="https://raw.githubusercontent.com/daiwk/dl.ai/master/c1/imgs/forward-propagation-for-layer-l.png" alt="forward-propagation-for-layer-l.png" />
<p class="caption">forward-propagation-for-layer-l.png</p>
</div>
<p>backward propagation for layer l:</p>
<p>参考<a href="https://github.com/daiwk/dl.ai/blob/master/c1/c1w3.md#backpropagation-intuition">C1W2的backward propagation intuition部分</a>： 注意：<span class="math inline"><em>d</em><em>a</em><sup>[<em>l</em>]</sup> * <em>g</em><sup>[<em>l</em>]′</sup>(<em>z</em><sup>[<em>l</em>]</sup>)</span>是element-wise product。</p>
<div class="figure">
<img src="https://raw.githubusercontent.com/daiwk/dl.ai/master/c1/imgs/backward-propagation-for-layer-l.png" alt="backward-propagation-for-layer-l.png" />
<p class="caption">backward-propagation-for-layer-l.png</p>
</div>
<p>对于最后一层L，如果是sigmoid并采用logloss，那么：</p>
<div class="figure">
<img src="https://raw.githubusercontent.com/daiwk/dl.ai/master/c1/imgs/summary-forward-backward-propagation.png" alt="summary-forward-backward-propagation.png" />
<p class="caption">summary-forward-backward-propagation.png</p>
</div>
<h2 id="parameters-vs-hyperparameters">parameters vs hyperparameters</h2>
<p>图中下方是：momentum，mini-batch size，regularization <img src="https://raw.githubusercontent.com/daiwk/dl.ai/master/c1/imgs/parameters-vs-hyperparameters.png" alt="summary-forward-backward-propagation.png" /></p>
<div class="figure">
<img src="https://raw.githubusercontent.com/daiwk/dl.ai/master/c1/imgs/apply-deep-learning-is-a-very-empirical-process.png" alt="apply-deep-learning-is-a-very-empirical-process.png" />
<p class="caption">apply-deep-learning-is-a-very-empirical-process.png</p>
</div>
<h2 id="what-does-this-have-to-do-with-the-brain">what does this have to do with the brain?</h2>
<div class="figure">
<img src="https://raw.githubusercontent.com/daiwk/dl.ai/master/c1/imgs/summary-and-brain.png" alt="summary-and-brain.png" />
<p class="caption">summary-and-brain.png</p>
</div>
<h2 id="others">others</h2>
<div class="figure">
<img src="https://raw.githubusercontent.com/daiwk/dl.ai/master/c1/imgs/shape-of-L-layer-nn.png" alt="shape-of-L-layer-nn.png" />
<p class="caption">shape-of-L-layer-nn.png</p>
</div>
