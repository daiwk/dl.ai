contents

  * [neural networks overview](#neural-networks-overview)
  * [neural network representation](#neural-network-representation)
  * [computing a neural network's output](#computing-a-neural-networks-output)
  * [vectorizzing across multiple examples](#vectorizzing-across-multiple-examples)
  * [explanation for vectorized implementation](#explanation-for-vectorized-implementation)
  * [activation functions](#activation-functions)
  * [why do you need non-linear activation functions](#why-do-you-need-non-linear-activation-functions)
  * [gradient descent for neural networks](#gradient-descent-for-neural-networks)
  * [backpropagation intuition](#backpropagation-intuition)
  * [random initialization](#random-initialization)


## neural networks overview

其中，每个神经元完成了$z=w^Tx+b$以及$a=\sigma (z)$两个操作($a$表示activation)，每一层的数据用上标[i]表示。

![neural-networks-overview](https://raw.githubusercontent.com/daiwk/dl.ai/master/c1/imgs/neural-networks-overview.png)

## neural network representation

2层nn（inputlayer不算在内，有hidden和output两层）
$shape(w^{[1]})=(4,3)$因为有4个神经元，输入是3维。同理$shape(b^{[1]})=(4,1)$。
而

## computing a neural network's output

## vectorizzing across multiple examples

## explanation for vectorized implementation

## activation functions

## why do you need non-linear activation functions

## gradient descent for neural networks

## backpropagation intuition

## random initialization


