contents

* [setting up your machine learning application](#setting-up-your-machine-learning-application)
  * [train/dev/test sets](#traindevtest-sets)
  * [bias/variance](#biasvariance)
  * [basic recipe for machine learning](#basic-recipe-for-machine-learning)
* [regularizing your neural network](#regularizing-your-neural-network)
  * [regularization](#regularization)
  * [why regularization reduces overfitting?](#why-regularization-reduces-overfitting)
  * [dropout regularization](#dropout-regularization)
  * [understanding dropout](#understanding-dropout)
  * [other regularization methods](#other-regularization-methods)
* [setting up your optimization problem](#setting-up-your-optimization-problem)
  * [normalizing inputs](#normalizing-inputs)
  * [vanishing/exploding gradients](#vanishingexploding-gradients)
  * [weight initialization for deep networks](#weight-initialization-for-deep-networks)
  * [numerical approximation of gradients](#numerical-approximation-of-gradients)
  * [gradient checking](#gradient-checking)
  * [gradient checking implementation notes](#gradient-checking-implementation-notes)

# setting up your machine learning application

## train/dev/test sets

传统机器学习，例如数据总量有1w，可以划分train:dev:test=70:0:30，或者，train:dev:test=60:20:20。
但对于大数据，例如100w的数据，那适当的比例应该是98:1:1或者甚至是99.5:0.25:0.25，或者99.5:0.4:0.1。

![train-dev-test-ratio.png](https://raw.githubusercontent.com/daiwk/dl.ai/master/c2/imgs/train-dev-test-ratio.png)

另外，训练集和验证集的分布要保持一致。测试集是为了no-bias地验证模型效果的，有些时候，可以甚至不要测试集，只要验证集就可以了。

![distribution-between-train-and-dev.png](https://raw.githubusercontent.com/daiwk/dl.ai/master/c2/imgs/distribution-between-train-and-dev.png)

## bias/variance
![bias-variance-under-overfitting.png](https://raw.githubusercontent.com/daiwk/dl.ai/master/c2/imgs/bias-variance-under-overfitting.png)
首先 Error = Bias + Variance
Error反映的是整个模型的准确度，Bias反映的是模型在样本上的输出与真实值之间的误差，即模型本身的精准度，Variance反映的是模型每一次输出结果与模型输出期望之间的误差，即模型的稳定性。(https://www.zhihu.com/question/27068705/answer/35151681)[https://www.zhihu.com/question/27068705/answer/35151681]
low-high-variance-bias的四象限如下：

![bias-variance-high-low.png](https://raw.githubusercontent.com/daiwk/dl.ai/master/c2/imgs/bias-variance-high-low.png)

对应模型的最优复杂度如下：
![bias-variance-high-low-complexity.png](https://raw.githubusercontent.com/daiwk/dl.ai/master/c2/imgs/bias-variance-high-low-complexity.png)

当optimal error(bayes error)约等于0%时（错误率是0，全部才能识别出来），如下图所示。当optimal error是15%时，第二个分类器就是low bias了。

![bias-variance-cat-classification.png](https://raw.githubusercontent.com/daiwk/dl.ai/master/c2/imgs/bias-variance-cat-classification.png)

最差情况就是两个都high的，如下紫色曲线（线性部分是high bias，因为有很多没分对的情况；中间两个点是high variance，overfitting了）:

![bias-variance-worst.png](https://raw.githubusercontent.com/daiwk/dl.ai/master/c2/imgs/bias-variance-worst.png)

## basic recipe for machine learning

# regularizing your neural network

## regularization

## why regularization reduces overfitting?

## dropout regularization

## understanding dropout

## other regularization methods

# setting up your optimization problem

## normalizing inputs

## vanishing/exploding gradients

## weight initialization for deep networks

## numerical approximation of gradients

## gradient checking

## gradient checking implementation notes



