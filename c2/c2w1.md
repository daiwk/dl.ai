contents

* [setting up your machine learning application](#setting-up-your-machine-learning-application)
  * [train/dev/test sets](#traindevtest-sets)
  * [bias/variance](#biasvariance)
  * [basic recipe for machine learning](#basic-recipe-for-machine-learning)
* [regularizing your neural network](#regularizing-your-neural-network)
  * [regularization](#regularization)
  * [why regularization reduces overfitting?](#why-regularization-reduces-overfitting)
  * [dropout regularization](#dropout-regularization)
  * [understanding dropout](#understanding-dropout)
  * [other regularization methods](#other-regularization-methods)
* [setting up your optimization problem](#setting-up-your-optimization-problem)
  * [normalizing inputs](#normalizing-inputs)
  * [vanishing/exploding gradients](#vanishingexploding-gradients)
  * [weight initialization for deep networks](#weight-initialization-for-deep-networks)
  * [numerical approximation of gradients](#numerical-approximation-of-gradients)
  * [gradient checking](#gradient-checking)
  * [gradient checking implementation notes](#gradient-checking-implementation-notes)

# setting up your machine learning application

## train/dev/test sets

传统机器学习，例如数据总量有1w，可以划分train:dev:test=70:0:30，或者，train:dev:test=60:20:20。
但对于大数据，例如100w的数据，那适当的比例应该是98:1:1或者甚至是99.5:0.25:0.25，或者99.5:0.4:0.1。

![train-dev-test-ratio.png](https://raw.githubusercontent.com/daiwk/dl.ai/master/c2/imgs/train-dev-test-ratio.png)

另外，训练集和验证集的分布要保持一致。测试集是为了no-bias地验证模型效果的，有些时候，可以甚至不要测试集，只要验证集就可以了。

![distribution-between-train-and-dev.png](https://raw.githubusercontent.com/daiwk/dl.ai/master/c2/imgs/distribution-between-train-and-dev.png)

## bias/variance
![bias-variance-under-overfitting.png](https://raw.githubusercontent.com/daiwk/dl.ai/master/c2/imgs/bias-variance-under-overfitting.png)
首先 Error = Bias + Variance
Error反映的是整个模型的准确度，Bias反映的是模型在样本上的输出与真实值之间的误差，即模型本身的精准度，Variance反映的是模型每一次输出结果与模型输出期望之间的误差，即模型的稳定性。(https://www.zhihu.com/question/27068705/answer/35151681)[https://www.zhihu.com/question/27068705/answer/35151681]
low-high-variance-bias的四象限如下：

![bias-variance-high-low.png](https://raw.githubusercontent.com/daiwk/dl.ai/master/c2/imgs/bias-variance-high-low.png)

对应模型的最优复杂度如下：
![bias-variance-high-low-complexity.png](https://raw.githubusercontent.com/daiwk/dl.ai/master/c2/imgs/bias-variance-high-low-complexity.png)

当optimal error(bayes error)约等于0%时（错误率是0，全部才能识别出来），如下图所示。当optimal error是15%时，第二个分类器就是low bias了。

![bias-variance-cat-classification.png](https://raw.githubusercontent.com/daiwk/dl.ai/master/c2/imgs/bias-variance-cat-classification.png)

最差情况就是两个都high的，如下紫色曲线（线性部分是high bias，因为有很多没分对的情况；中间两个点是high variance，overfitting了）:

![bias-variance-worst.png](https://raw.githubusercontent.com/daiwk/dl.ai/master/c2/imgs/bias-variance-worst.png)

## basic recipe for machine learning

首先，如果你的模型有high bias(在训练集上表现很差)，那么，试着用更大的网络（更复杂的模型），或者，训练更久。

如果bias已经变小了，那么看看是否是high viarance(从训练集到验证集的表现的变化)，如果是，那么可以尝试获取更多数据/正则化。

在传统机器学习领域，有bias-variance tradeoff的说法，因为往往优化bias，会让viarance变差，反之亦然。

但在deep learning中，上述方法在提高一个指标的时候，往往并不会太影响另一个指标。所以，如果你已经使用了正则化，那么，使用更大的模型几乎不会有什么负面影响，造成的影响只是计算量的增加而已（Training a bigger network almost never hurts. And the main cost of training a neural network that's too big is just computational time, so long as you're regularizing. ）

![basic-recipe-for-machine-learning.png](https://raw.githubusercontent.com/daiwk/dl.ai/master/c2/imgs/basic-recipe-for-machine-learning.png)

# regularizing your neural network

## regularization

在lr中，一般只对w加正则，因为b是一个实数，而w的维度较高(`\(n_x\)`)，所以b的影响可以忽略不计。

L0范数是指向量中非0的元素的个数，如果我们用L0范数来规则化一个参数矩阵W的话，就是希望W的大部分元素都是0，也就是，让参数W是稀疏的。

为什么不用L0，而要用L1呢？个人理解一是因为L0范数很难优化求解（NP难问题），二是L1范数是L0范数的最优凸近似，而且它比L0范数要容易优化求解。

l1(也叫Lasso regularization)可以使模型变得更sparse[参考[http://blog.csdn.net/zouxy09/article/details/24971995](http://blog.csdn.net/zouxy09/article/details/24971995)]，因为**它是L0范数的最优凸近似，任何的规则化算子，如果他在`\(W_i=0\)`的地方不可微，并且可以分解为一个“求和”的形式，那么这个规则化算子就可以实现稀疏。这说是这么说，W的L1范数是绝对值，|w|在w=0处是不可微**

稀疏的两个好处：
+ 特征选择(Feature Selection)

一般来说，xi的大部分元素（也就是特征）都是和最终的输出yi没有关系或者不提供任何信息的，在最小化目标函数的时候考虑xi这些额外的特征，虽然可以获得更小的训练误差，但在预测新的样本时，这些没用的信息反而会被考虑，从而干扰了对正确yi的预测。稀疏规则化算子的引入就是为了完成特征自动选择的光荣使命，它会学习地去掉这些没有信息的特征，也就是把这些特征对应的权重置为0。

+ 可解释性(Interpretability)

例如患某种病的概率是y，然后我们收集到的数据x是1000维的，也就是我们需要寻找这1000种因素到底是怎么影响患上这种病的概率的。假设我们这个是个回归模型：`\(y=w1*x1+w2*x2+…+w1000*x1000+b\)`（当然了，为了让y限定在[0,1]的范围，一般还得加个Logistic函数）。通过学习，如果最后学习到的`\(w^*\)`就只有很少的非零元素，例如只有5个非零的wi，那么我们就有理由相信，这些对应的特征在患病分析上面提供的信息是巨大的，决策性的。也就是说，患不患这种病只和这5个因素有关，那医生就好分析多了。但如果1000个wi都非0，医生面对这1000种因素，累觉不爱。

在神经网络的训练中，l1用处不大，l2用得很广泛。

其中的`\(\lambda\)`是正则化参数，一般通过验证集或者cross-validation来设置。

![regression-lr.png](https://raw.githubusercontent.com/daiwk/dl.ai/master/c2/imgs/regression-lr.png)

神经网络的l2里，范数是F-范数**的平方**（**F-范数=2-范数=矩阵中所有元素的平方和再开平方**），也叫做weight decay(因为相当于给w乘上了一个小于1的因子：`\((1-\alpha \frac{\lambda }{m})\)`)。

用上l2的回归也叫“岭回归”（Ridge Regression），能够防止过拟合。让L2范数的规则项||W||2最小，可以使得W的每个元素都很小，都接近于0，但与L1范数不同，它不会让它等于0，而是接近于0。越小的参数说明模型越简单，越简单的模型则越不容易产生过拟合现象。我也不懂，我的理解是：限制了参数很小，实际上就限制了多项式某些分量的影响很小（看上面线性回归的模型的那个拟合的图），这样就相当于减少参数个数。

l2的好处：
+ 学习理论的角度：

从学习理论的角度来说，L2范数可以防止过拟合，提升模型的泛化能力。

+ 优化计算的角度：

从优化或者数值计算的角度来说，L2范数有助于处理 condition number不好的情况下矩阵求逆很困难的问题。

优化有两大难题，一是：局部最小值，二是：ill-condition病态问题。

ill-condition对应的是well-condition。那他们分别代表什么？假设我们有个方程组AX=b，我们需要求解X。如果A或者b稍微的改变，会使得X的解发生很大的改变，那么这个方程组系统就是ill-condition的，反之就是well-condition的。condition number就是拿来衡量ill-condition系统的可信度的。condition number衡量的是输入发生微小变化的时候，输出会发生多大的变化。也就是系统对微小变化的敏感度。如果一个矩阵的condition number在1附近，那么它就是well-conditioned的，如果远大于1，那么它就是ill-conditioned的。

![ill-conditioned.png](https://raw.githubusercontent.com/daiwk/dl.ai/master/c2/imgs/ill-conditioned.png)

l2范数如何解决ill-conditioned问题：

![l2-improve-condition-number.png](https://raw.githubusercontent.com/daiwk/dl.ai/master/c2/imgs/l2-improve-condition-number.png)


![neural-network-lr.png](https://raw.githubusercontent.com/daiwk/dl.ai/master/c2/imgs/neural-network-lr.png)

## why regularization reduces overfitting?

## dropout regularization

## understanding dropout

## other regularization methods

# setting up your optimization problem

## normalizing inputs

## vanishing/exploding gradients

## weight initialization for deep networks

## numerical approximation of gradients

## gradient checking

## gradient checking implementation notes



