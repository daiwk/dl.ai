contents

  * [mini-batch gradient descent](#mini-batch-gradient-descent)
  * [understanding mini-batch gradient descent](#understanding-mini-batch-gradient-descent)
  * [exponentially weighted averages](#exponentially-weighted-averages)
  * [understanding exponentially weighted averages](#understanding-exponentially-weighted-averages)
  * [bias correction in exponentially weighted averages](#bias-correction-in-exponentially-weighted-averages)
  * [gradient descent with momentum](#gradient-descent-with-momentum)
  * [RMSprop](#rmsprop)
  * [Adam optimization algorithm](#adam-optimization-algorithm)
  * [learning rate decay](#learning-rate-decay)
  * [the problem of local optima](#the-problem-of-local-optima)

## mini-batch gradient descent

+ batch gradient descent：遍历所有训练样本
+ mini-batch gradient descent：一次处理一个min-batch

各符号的表示：

+ $X^{(i)}$表示第i个训练样本
+ $z^{[l]}$表求第l层的激活函数的输入
+ $X^{\{t\}},Y^{\{t\}}$表示第t个mini-batch的样本

![mini-batch-gradient-descent-introduction.png](https://raw.githubusercontent.com/daiwk/dl.ai/master/c2/imgs/mini-batch-gradient-descent-introduction.png)

假设有5000000个样本，那么在一个epoch中，遍历5000个mini-batch，每个mini-batch（1000个样本）中，先前向计算，然后算这1000个样本的loss，然后反向传播。

注意：一个mini-batch就反射传播一次，更新一次参数，所以一个epoch就更新了5000次参数。

![mini-batch-gradient-descent.png](https://raw.githubusercontent.com/daiwk/dl.ai/master/c2/imgs/mini-batch-gradient-descent.png)

## understanding mini-batch gradient descent

可见，使用mini-batch的cost会有不小的抖动和噪音。

![mini-batch-gradient-descent-cost.png](https://raw.githubusercontent.com/daiwk/dl.ai/master/c2/imgs/mini-batch-gradient-descent-cost.png)

+ batchsize=m: batch gradient descent。一般可以顺利到达min。但耗时长。
+ batchsize=1: stochastic gradient descent。很可能在min附近徘徊。而且失去了vectorization可以带来了加速
+ batchsize between 1,m: 
  + 充分利用vectorization
  + make progress without processing entire training set

![mini-batch-gradient-descent-choose-batchsize.png](https://raw.githubusercontent.com/daiwk/dl.ai/master/c2/imgs/mini-batch-gradient-descent-choose-batchsize.png)

+ 训练集很小时（m<=2000）：使用batch gradient descent
+ 其他情况下，经典的batchsize：2的指数（64,128,256,512），一般比较少用1024，非2的指数也比较少用（如1000）
+ make sure mini-batch里的$X^{\{t\}},Y^{\{t\}}$能够在cpu/gpu中被容纳下

![mini-batch-gradient-descent-choose-size.png](https://raw.githubusercontent.com/daiwk/dl.ai/master/c2/imgs/mini-batch-gradient-descent-choose-size.png)

## exponentially weighted averages

## understanding exponentially weighted averages

## bias correction in exponentially weighted averages

## gradient descent with momentum

## RMSprop

## Adam optimization algorithm

## learning rate decay

## the problem of local optima


