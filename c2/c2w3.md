contents

<!-- TOC -->

- [1. hyperparameter tuning](#1-hyperparameter-tuning)
    - [1.1. tuning process](#11-tuning-process)
    - [1.2. using an appropriate scale to pick hyperparameters](#12-using-an-appropriate-scale-to-pick-hyperparameters)
    - [1.3. hyperparameters tuning in practice: Pandas vs. Caviar](#13-hyperparameters-tuning-in-practice-pandas-vs-caviar)
- [2. batch normalization](#2-batch-normalization)
    - [2.1. normalizing activations in a network](#21-normalizing-activations-in-a-network)
    - [2.2. fitting batch norm into a neural network](#22-fitting-batch-norm-into-a-neural-network)
    - [2.3. why does batch norm work?](#23-why-does-batch-norm-work)
    - [2.4. batch norm at test time](#24-batch-norm-at-test-time)
- [3. multi-class classification](#3-multi-class-classification)
    - [3.1. softmax regression](#31-softmax-regression)
    - [3.2. training a softmax classifier](#32-training-a-softmax-classifier)
- [4. introduction to programming frameworks](#4-introduction-to-programming-frameworks)
    - [4.1. deep learning frameworks](#41-deep-learning-frameworks)
    - [4.2. tensorflow](#42-tensorflow)

<!-- /TOC -->

# 1. hyperparameter tuning

## 1.1. tuning process

如图，各超参的优先级为红>黄>紫，而adam的三个参数基本都不调整。

![hyperparameters.png](https://raw.githubusercontent.com/daiwk/dl.ai/master/c2/imgs/hyperparameters.png)

当参数比较少时，可以用表格，两两组合去尝试，但如果超参很多，可以用随机组合，因为事先并不知道各个参数的重新程度。例如，用一个很重要的参数$\alpha$和一个没那么重要的参数$\epsilon$组合，可能会发现试完25种组合，不论$\epsilon$取什么值，效果都基本只和$\alpha$的取值有关。而如果用随机组合的方法，同样的25个模型，可以试25种$\alpha$的取值。

![tuning-process-try-random-values.png](https://raw.githubusercontent.com/daiwk/dl.ai/master/c2/imgs/tuning-process-try-random-values.png)

coarse to fine(由粗到精)原则：当发现某个空间里的超参组合表现比较好时，例如图中蓝圈的那几个点，那么，可以在一个小区域（图中的蓝色正方形）内sample更多的参数组合。

![tuning-process-coarse-to-fine.png](https://raw.githubusercontent.com/daiwk/dl.ai/master/c2/imgs/tuning-process-coarse-to-fine.png)

## 1.2. using an appropriate scale to pick hyperparameters

一种方法是根据均匀分布(uniform distribution)来随机地选参数值：

![pick-hyperparameters-uniformly.png](https://raw.githubusercontent.com/daiwk/dl.ai/master/c2/imgs/pick-hyperparameters-uniformly.png)

但，如果$\alpha$从0.0001到1之间均匀地随机采样，会有90%的值落在0.1-1之间，而0.001到0.1之间只有10%的值。相当于用了90%的资源搜索0.1-1的值，但只用10%的资源搜索0.0001-0.1的值。

更合理的方法是，在log scale上进行搜索，如图中的第二个数轴，每个区域的起点和终点是10倍的关系。

起点是$10^a$，终点是$10^b$，而$0.0001=10^a$，所以，$a=log_{10}0.0001=-4$。所以

```python
r = -4 * np.random.rand() # -4 <= r <= 0
alpha = 10 ** r # 10^-4 <= alpha <= 1
```

![pick-hyperparameters-log-scale.png](https://raw.githubusercontent.com/daiwk/dl.ai/master/c2/imgs/pick-hyperparameters-log-scale.png)

而对于exponentially weight average的超参$\beta$，取值范围从0.9(相当于10天的平均)，到0.999（相当于1000天的平均）。这种情况，就做一个变换，对$(1-\beta)$在log scale上进行采样就行了。

特别地，**对于exponentially weight average而言，不能使用简单的均匀采样，要用log scale的采样。**因为，**当$\beta$的取值趋近于1时，$\beta$的微小改变，会导致结果特别敏感**。取$\beta$时，相当于$1/(1-\beta)$天的平均：
+ 当$\beta$比较小时（如0.9000变化成0.9005），这个变化就是从10.0天到10.05的变化
+ 而$\beta$比较大时（如0.9990变化成0.999），那这个变化就是从1/(1-0.999)=1000天变到1/(1-0.9995)=2000天了！

![pick-hyperparameters-exponetially-weighted-average.png](https://raw.githubusercontent.com/daiwk/dl.ai/master/c2/imgs/pick-hyperparameters-exponetially-weighted-average.png)

## 1.3. hyperparameters tuning in practice: Pandas vs. Caviar

深度学习可以用在不同领域，而各领域的经验其实可以相互借鉴，所以intuitions do get stale(直觉会变得陈腐，需要与时俱进)，所以至少每几个月都最好调整下超参，保证总是可以得到最优解。

![retest-hyperparameters-occatsionally.png](https://raw.githubusercontent.com/daiwk/dl.ai/master/c2/imgs/retest-hyperparameters-occatsionally.png)

有以下主要两种流派：

1. 精心照料一个模型：当没有足够的资源同时训练多个模型时，会对一个模型天天去微调参数或者发现出问题的话，回滚到上一个checkpoint，重新调参（panda approach，熊猫产子少…）
2. 并行训练多个模型：尝试不同的超参组合，选择一个最好的模型（caviar strategy,鱼子酱，鱼产子多…）

![pick-hyperparameters-two-approaches.png](https://raw.githubusercontent.com/daiwk/dl.ai/master/c2/imgs/pick-hyperparameters-two-approaches.png)

# 2. batch normalization

## 2.1. normalizing activations in a network

normalizing input values在前面提到过(c2w1的3.1部分)，对lr很有效。

如果是多层神经网络，例如图中，想把$W^[3]$,$b^[3]$训练得更好，应该也对$a^[2]$进行normalize。事实上，batch norm就做的这件事，只是区别在于，bn的normalize针对的是$z^[2]$，而非$a^[2]$。在学术界上，对$a^[2]$还是$z^[2]$进行normalize是有争议的，但对$z^[2]$的normalize比较普遍。

![bn-problems.png](https://raw.githubusercontent.com/daiwk/dl.ai/master/c2/imgs/bn-problems.png)

给定一系列的中间值$z^{(1)},z^{(2)},...z^{(m)}$，这里$z^{(i)}$是$z^{[l]\{i\}}$的简写。

经过如下变换，得到0均值，1方差的$z_{norm}^{(i)}$：
$$\mu=\frac{1}{m}\sum_iz^{i}$$
$$\sigma^2=\frac{1}{m}\sum_i(z^{(i)}-\mu)^2$$
$$z_{norm}^{(i)}=\frac{z^{(i)}-\mu}{\sqrt{\sigma^2+\epsilon}}$$

注意，其中的$\epsilon$是为了防止分母为0。而我们并不希望所有$z^{(i)}$都变成0均值1方差的结果，也许它们本身的分布就不同[如图中，假设激活函数是sigmoid，并不希望数值是0均值1方差的，因为需要值很大或者很小，这样才能充分利用sigmoid的非线性]，所以，可以做如下处理：

$$ \tilde{z}^{(i)}=\gamma z_{norm}^{(i)}+\beta$$

而其中的$\gamma$和$\beta$是像$W$和$b$一样的可以学习的参数

如果
$$\gamma=\sqrt{\sigma^2+\epsilon}$$
$$\beta=\mu$$
那么，$\tilde{z}^{(i)}=z^{(i)}$

所以，不同层使用不同的$\gamma$和$\beta$，可以使各层的分布不一样。最终使用$\tilde{z}^{[l](i)}$代替$z^{[l](i)}$

![bn-problems.png](https://raw.githubusercontent.com/daiwk/dl.ai/master/c2/imgs/bn-problems.png)

## 2.2. fitting batch norm into a neural network

![bn-adding-bn-to-a-nn.png](https://raw.githubusercontent.com/daiwk/dl.ai/master/c2/imgs/bn-adding-bn-to-a-nn.png)

![bn-adding-bn-to-a-nn.png](../c2/imgs/bn-adding-bn-to-a-nn.png)



## 2.3. why does batch norm work?

## 2.4. batch norm at test time

# 3. multi-class classification

## 3.1. softmax regression

## 3.2. training a softmax classifier

# 4. introduction to programming frameworks

## 4.1. deep learning frameworks

## 4.2. tensorflow

