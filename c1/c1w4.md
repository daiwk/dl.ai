contents

<!-- TOC -->

- [deep L-layer neural network](#deep-l-layer-neural-network)
- [forward propagation in a deep network](#forward-propagation-in-a-deep-network)
- [getting your matrix dimensions right](#getting-your-matrix-dimensions-right)
- [why deep representations?](#why-deep-representations)
- [building blocks of deep neural networks](#building-blocks-of-deep-neural-networks)
- [forward and backward propagation](#forward-and-backward-propagation)
- [parameters vs hyperparameters](#parameters-vs-hyperparameters)
- [what does this have to do with the brain?](#what-does-this-have-to-do-with-the-brain)
- [others](#others)

<!-- /TOC -->

## deep L-layer neural network

![deep-neural-network-notation.png](https://raw.githubusercontent.com/daiwk/dl.ai/master/c1/imgs/deep-neural-network-notation.png)

## forward propagation in a deep network

$z^{[l](i)}$表示第l层的第i个训练样本(列向量)，$Z^{[l]}$表示将第l层的这$m$个训练样本全部水平地放在一起形成的矩阵。以此vectorization的方法，可以避免从1->$m$的这个for-loop。

另外，从第1层到第4层这个for-loop是无法避免的。

![forward-propagation-in-a-deep-network.png](https://raw.githubusercontent.com/daiwk/dl.ai/master/c1/imgs/forward-propagation-in-a-deep-network.png)

## getting your matrix dimensions right

![w-and-b-dimension.png](https://raw.githubusercontent.com/daiwk/dl.ai/master/c1/imgs/w-and-b-dimension.png)

vectorized之后，$b^{[l]}$仍然是$(n^{[l]},1)$维，只是因为broadcasting，才复制m遍，变成了$(n^{[l]},m)$维。

![vectorized-implementation.png](https://raw.githubusercontent.com/daiwk/dl.ai/master/c1/imgs/vectorized-implementation.png)

## why deep representations?

![intuition-of-deep-representation.png](https://raw.githubusercontent.com/daiwk/dl.ai/master/c1/imgs/intuition-of-deep-representation.png)

对于$(x_1,x_2,...x_n)$的异或（XOR）操作，如果用树型结构，n个叶子节点，则树深度是$log_2n+1$(深度为k的满二叉树的第i层上有$2^{i-1}$个结点,总共至多有$2^k-1$个结点)，即只需要$O(log_2n)$层的树就能完成。

而如果采用单隐层，则需要$2^{(n-1)}$个节点

![circuit-theory-and-deep-learning.png](https://raw.githubusercontent.com/daiwk/dl.ai/master/c1/imgs/circuit-theory-and-deep-learning.png)

## building blocks of deep neural networks

forward时，需要cache $Z^{[l]}$以供backward使用。

![forward-and-backward-functions.png](https://raw.githubusercontent.com/daiwk/dl.ai/master/c1/imgs/forward-and-backward-functions.png)

为了计算backward，其实需要cache的有$Z^{[l]}$、$W^{[l]}$以及$b^{[l]}$：

![forward-backward-functions.png](https://raw.githubusercontent.com/daiwk/dl.ai/master/c1/imgs/forward-backward-functions.png)

## forward and backward propagation

forward propagation for layer l:

![forward-propagation-for-layer-l.png](https://raw.githubusercontent.com/daiwk/dl.ai/master/c1/imgs/forward-propagation-for-layer-l.png)

backward propagation for layer l:

参考[C1W2的backward propagation intuition部分](https://github.com/daiwk/dl.ai/blob/master/c1/c1w3.md#backpropagation-intuition)：
注意：$da^{[l]}*g^{[l]'}(z^{[l]})$是element-wise product。

![backward-propagation-for-layer-l.png](https://raw.githubusercontent.com/daiwk/dl.ai/master/c1/imgs/backward-propagation-for-layer-l.png)

对于最后一层L，如果是sigmoid并采用logloss，那么：

![summary-forward-backward-propagation.png](https://raw.githubusercontent.com/daiwk/dl.ai/master/c1/imgs/summary-forward-backward-propagation.png)

## parameters vs hyperparameters

图中下方是：momentum，mini-batch size，regularization
![summary-forward-backward-propagation.png](https://raw.githubusercontent.com/daiwk/dl.ai/master/c1/imgs/parameters-vs-hyperparameters.png)

![apply-deep-learning-is-a-very-empirical-process.png](https://raw.githubusercontent.com/daiwk/dl.ai/master/c1/imgs/apply-deep-learning-is-a-very-empirical-process.png)

## what does this have to do with the brain?

![summary-and-brain.png](https://raw.githubusercontent.com/daiwk/dl.ai/master/c1/imgs/summary-and-brain.png)

## others

![shape-of-L-layer-nn.png](https://raw.githubusercontent.com/daiwk/dl.ai/master/c1/imgs/shape-of-L-layer-nn.png)
