contents

  * [mini-batch gradient descent](#mini-batch-gradient-descent)
  * [understanding mini-batch gradient descent](#understanding-mini-batch-gradient-descent)
  * [exponentially weighted averages](#exponentially-weighted-averages)
  * [understanding exponentially weighted averages](#understanding-exponentially-weighted-averages)
  * [bias correction in exponentially weighted averages](#bias-correction-in-exponentially-weighted-averages)
  * [gradient descent with momentum](#gradient-descent-with-momentum)
  * [RMSprop](#rmsprop)
  * [Adam optimization algorithm](#adam-optimization-algorithm)
  * [learning rate decay](#learning-rate-decay)
  * [the problem of local optima](#the-problem-of-local-optima)

## mini-batch gradient descent

+ batch gradient descent：遍历所有训练样本
+ mini-batch gradient descent：一次处理一个min-batch

各符号的表示：

+ $X^{(i)}$表示第i个训练样本
+ $z^{[l]}$表求第l层的激活函数的输入
+ $X^{\{t \}},Y^{\{t \}}$表示第t个mini-batch的样本

![mini-batch-gradient-descent-introduction.png](https://raw.githubusercontent.com/daiwk/dl.ai/master/c2/imgs/mini-batch-gradient-descent-introduction.png)

假设有5000000个样本，那么在一个epoch中，遍历5000个mini-batch，每个mini-batch（1000个样本）中，先前向计算，然后算这1000个样本的loss，然后反向传播。

注意：一个mini-batch就反射传播一次，更新一次参数，所以一个epoch就更新了5000次参数。

![mini-batch-gradient-descent.png](https://raw.githubusercontent.com/daiwk/dl.ai/master/c2/imgs/mini-batch-gradient-descent.png)

## understanding mini-batch gradient descent

可见，使用mini-batch的cost会有不小的抖动和噪音。

![mini-batch-gradient-descent-cost.png](https://raw.githubusercontent.com/daiwk/dl.ai/master/c2/imgs/mini-batch-gradient-descent-cost.png)

+ batchsize=m: batch gradient descent。一般可以顺利到达min。但耗时长。
+ batchsize=1: stochastic gradient descent。很可能在min附近徘徊。而且失去了vectorization可以带来了加速
+ batchsize between 1,m: 
  + 充分利用vectorization
  + make progress without processing entire training set

![mini-batch-gradient-descent-choose-batchsize.png](https://raw.githubusercontent.com/daiwk/dl.ai/master/c2/imgs/mini-batch-gradient-descent-choose-batchsize.png)

+ 训练集很小时（m<=2000）：使用batch gradient descent
+ 其他情况下，经典的batchsize：2的指数（64,128,256,512），一般比较少用1024，非2的指数也比较少用（如1000）
+ make sure mini-batch里的$X^{\{t\}},Y^{\{t\}}$能够在cpu/gpu中被容纳下

![mini-batch-gradient-descent-choose-size.png](https://raw.githubusercontent.com/daiwk/dl.ai/master/c2/imgs/mini-batch-gradient-descent-choose-size.png)

## exponentially weighted averages

比gradient descent更快的算法大多使用指数平滑类的算法。通过图中$v_t$的计算，可以得到图中的红线，即exponentially weighted (moving) average

![exponential-weight-average-introduction.png](https://raw.githubusercontent.com/daiwk/dl.ai/master/c2/imgs/exponential-weight-average-introduction.png)

$$v_t=\beta v_{t-1}+(1-\beta)\theta_t$$

**注意，$\beta$对应的是$v_{t-1}$，而$1-\beta$对应的是$\theta_t$。**

而，曲线描绘了过去大约$\frac{1}{1-\beta}$天的变量取值的平均情况，所以，$\beta$越大，窗口越大，曲线越滞后，对变量的变化也越不敏感。图中，红线是0.9，绿线是0.98，黄线是0.5。

![exponential-weight-average-formula.png](https://raw.githubusercontent.com/daiwk/dl.ai/master/c2/imgs/exponential-weight-average-formula.png)

## understanding exponentially weighted averages

$(1-\epsilon)^{1/\epsilon}$约等于$1/e$，而$\epsilon=1-\beta$。所以是${\beta}^{1/(1-\beta)}$，所以前面提到，大约是$1/(1-\beta)$天

![exponential-weight-average-analyze.png](https://raw.githubusercontent.com/daiwk/dl.ai/master/c2/imgs/exponential-weight-average-analyze.png)

好处：
+ 计算简单
+ 省内存（只需要存一个变量v就行了）

![exponential-weight-average-implementation.png](https://raw.githubusercontent.com/daiwk/dl.ai/master/c2/imgs/exponential-weight-average-implementation.png)

## bias correction in exponentially weighted averages

## gradient descent with momentum

## RMSprop

## Adam optimization algorithm

## learning rate decay

## the problem of local optima


