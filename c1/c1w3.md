contents

  * [neural networks overview](#neural-networks-overview)
  * [neural network representation](#neural-network-representation)
  * [computing a neural network's output](#computing-a-neural-networks-output)
  * [vectorizing across multiple examples](#vectorizing-across-multiple-examples)
  * [explanation for vectorized implementation](#explanation-for-vectorized-implementation)
  * [activation functions](#activation-functions)
  * [why do you need non-linear activation functions](#why-do-you-need-non-linear-activation-functions)
  * [gradient descent for neural networks](#gradient-descent-for-neural-networks)
  * [backpropagation intuition](#backpropagation-intuition)
  * [random initialization](#random-initialization)


## neural networks overview

其中，每个神经元完成了$z=w^Tx+b$以及$a=\sigma (z)$两个操作($a$表示activation)，每一层的数据用上标[i]表示。

![neural-networks-overview](https://raw.githubusercontent.com/daiwk/dl.ai/master/c1/imgs/neural-networks-overview.png)

## neural network representation

图示是一个2层nn（inputlayer不算在内，有hidden和output两层）。

如果输入的x有3维，在lr中，$shape(w)=(1,3)$，$shape(b)=(1,1)$。

而在nn中，$shape(w^{[1]})=(4,3)$因为有4个神经元，输入是3维。同理$shape(b^{[1]})=(4,1)$。

而$shape(w^{[2]})=(1,4)$，因为只有1个神经元，输入是3维。同理$shape(b^{[2]})=(1,1)$。

![neural-network-representation](https://raw.githubusercontent.com/daiwk/dl.ai/master/c1/imgs/neural-network-representation.png)

## computing a neural network's output

![compute-nn-output-1](https://raw.githubusercontent.com/daiwk/dl.ai/master/c1/imgs/compute-nn-output-1.png)
![compute-nn-output-1](https://raw.githubusercontent.com/daiwk/dl.ai/master/c1/imgs/compute-nn-output-2.png)

## vectorizing across multiple examples

![vectorizing-across-multiple-examples-for-loop](https://raw.githubusercontent.com/daiwk/dl.ai/master/c1/imgs/vectorizing-across-multiple-examples-for-loop.png)

矩阵$X$纵向是x的维数（行数），横向是training examples的个数（列数）。

矩阵$Z$、$A$纵向是hidden units的个数（行数），横向是training examples的个数（列数）。

![vectorizing-across-multiple-examples-vectorization](https://raw.githubusercontent.com/daiwk/dl.ai/master/c1/imgs/vectorizing-across-multiple-examples-vectorization.png)

## explanation for vectorized implementation

![justification-for-vectorized-implementation](https://raw.githubusercontent.com/daiwk/dl.ai/master/c1/imgs/justification-for-vectorized-implementation.png)
![vectorizing-across-multiple-examples-recap](https://raw.githubusercontent.com/daiwk/dl.ai/master/c1/imgs/vectorizing-across-multiple-examples-recap.png)

## activation functions

一般来说，$tanh$效果比$sigmoid$好，因为均值是0。但对于outputlayer而言，一般$y \in \\{0,1\\}$，所以希望$0\le \hat y \le 1$，所以会用$sigmoid$。

## why do you need non-linear activation functions

## gradient descent for neural networks

## backpropagation intuition

## random initialization


