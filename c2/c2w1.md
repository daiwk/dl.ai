contents

- [setting up your machine learning application](#setting-up-your-machine-learning-application)
  - [train/dev/test sets](#traindevtest-sets)
  - [bias/variance](#biasvariance)
  - [basic recipe for machine learning](#basic-recipe-for-machine-learning)
- [regularizing your neural network](#regularizing-your-neural-network)
  - [regularization](#regularization)
  - [why regularization reduces overfitting?](#why-regularization-reduces-overfitting)
  - [dropout regularization](#dropout-regularization)
  - [understanding dropout](#understanding-dropout)
  - [other regularization methods](#other-regularization-methods)
- [setting up your optimization problem](#setting-up-your-optimization-problem)
  - [normalizing inputs](#normalizing-inputs)
  - [vanishing/exploding gradients](#vanishingexploding-gradients)
  - [weight initialization for deep networks](#weight-initialization-for-deep-networks)
  - [numerical approximation of gradients](#numerical-approximation-of-gradients)
  - [gradient checking](#gradient-checking)
  - [gradient checking implementation notes](#gradient-checking-implementation-notes)

# setting up your machine learning application

## train/dev/test sets

传统机器学习，例如数据总量有1w，可以划分train:dev:test=70:0:30，或者，train:dev:test=60:20:20。
但对于大数据，例如100w的数据，那适当的比例应该是98:1:1或者甚至是99.5:0.25:0.25，或者99.5:0.4:0.1。

![train-dev-test-ratio.png](https://raw.githubusercontent.com/daiwk/dl.ai/master/c2/imgs/train-dev-test-ratio.png)

另外，训练集和验证集的分布要保持一致。测试集是为了no-bias地验证模型效果的，有些时候，可以甚至不要测试集，只要验证集就可以了。

![distribution-between-train-and-dev.png](https://raw.githubusercontent.com/daiwk/dl.ai/master/c2/imgs/distribution-between-train-and-dev.png)

## bias/variance
![bias-variance-under-overfitting.png](https://raw.githubusercontent.com/daiwk/dl.ai/master/c2/imgs/bias-variance-under-overfitting.png)
首先 Error = Bias + Variance

Error反映的是整个模型的准确度，Bias反映的是模型在样本上的输出与真实值之间的误差，即模型本身的精准度，Variance反映的是模型每一次输出结果与模型输出期望之间的误差，即模型的稳定性。[https://www.zhihu.com/question/27068705/answer/35151681](https://www.zhihu.com/question/27068705/answer/35151681)

low-high-variance-bias的四象限如下：

![bias-variance-high-low.png](https://raw.githubusercontent.com/daiwk/dl.ai/master/c2/imgs/bias-variance-high-low.png)

对应模型的最优复杂度如下：
![bias-variance-high-low-complexity.png](https://raw.githubusercontent.com/daiwk/dl.ai/master/c2/imgs/bias-variance-high-low-complexity.png)

当optimal error(bayes error)约等于0%时（错误率是0，全部才能识别出来），如下图所示。当optimal error是15%时，第二个分类器就是low bias了。

![bias-variance-cat-classification.png](https://raw.githubusercontent.com/daiwk/dl.ai/master/c2/imgs/bias-variance-cat-classification.png)

最差情况就是两个都high的，如下紫色曲线（线性部分是high bias，因为有很多没分对的情况；中间两个点是high variance，overfitting了）:

![bias-variance-worst.png](https://raw.githubusercontent.com/daiwk/dl.ai/master/c2/imgs/bias-variance-worst.png)

## basic recipe for machine learning

首先，如果你的模型有high bias(在训练集上表现很差)，那么，试着用更大的网络（更复杂的模型），或者，训练更久。

如果bias已经变小了，那么看看是否是high viarance(从训练集到验证集的表现的变化)，如果是，那么可以尝试获取更多数据/正则化。

在传统机器学习领域，有bias-variance tradeoff的说法，因为往往优化bias，会让viarance变差，反之亦然。

但在deep learning中，上述方法在提高一个指标的时候，往往并不会太影响另一个指标。所以，如果你已经使用了正则化，那么，使用更大的模型几乎不会有什么负面影响，造成的影响只是计算量的增加而已（Training a bigger network almost never hurts. And the main cost of training a neural network that's too big is just computational time, so long as you're regularizing. ）

![basic-recipe-for-machine-learning.png](https://raw.githubusercontent.com/daiwk/dl.ai/master/c2/imgs/basic-recipe-for-machine-learning.png)

# regularizing your neural network

## regularization

在lr中，一般只对w加正则，因为b是一个实数，而w的维度较高$n_x$，所以b的影响可以忽略不计。

![regression-lr.png](https://raw.githubusercontent.com/daiwk/dl.ai/master/c2/imgs/regression-lr.png)

在神经网络的训练中，l1用处不大，l2用得很广泛。其中的$\lambda$是正则化参数，一般通过验证集或者cross-validation来设置。

神经网络的l2里，范数是F-范数**的平方**（**F-范数=2-范数=矩阵中所有元素的平方和再开平方**），也叫做weight decay(因为在对w进行梯度下降时，相当于给w乘上了一个小于1的因子：$(1-\alpha \frac{\lambda }{m})$)。

![neural-network-lr.png](https://raw.githubusercontent.com/daiwk/dl.ai/master/c2/imgs/neural-network-lr.png)

## why regularization reduces overfitting?

直观地看，当$\lambda$很大时，最小化loss，会使得w趋向0，这样，相当于很多神经元无形地被干掉了，模型变简单了。

![how-does-regularization-reduces-overfitting.png](https://raw.githubusercontent.com/daiwk/dl.ai/master/c2/imgs/how-does-regularization-reduces-overfitting.png)

接下来，w接近0，所以z也比较小，而如果激活函数是tanh(z)，那么，tanh(z)的这个区域里是接近线性的，所以模型就更像一个比较大的线性回归。

另外，因为加了正则化项，所以原来的J可能不会在每个elevations(调幅数量？？看着又像iteration..)都单调递减，要看新的J

![how-does-regularization-reduces-overfitting-2.png](https://raw.githubusercontent.com/daiwk/dl.ai/master/c2/imgs/how-does-regularization-reduces-overfitting-2.png)

## dropout regularization

简单理解，dropout就是对每一层设置一个dropout rate，在训练时，针对每一条训练样本，以这个比例把某些神经元及与其连接的权重直接去掉。

![dropout-regularization-introduction.png](https://raw.githubusercontent.com/daiwk/dl.ai/master/c2/imgs/dropout-regularization-introduction.png)

dropout的实现方式：
keep_prob=0.8，表示dropout_rate=0.2
第三行的a3/=keep_prob，就是inverted dropout的精髓，这是为了让a3的期望和没有做dropout保持一致。加了这句话，在test的时候就更加容易了，在早期的实现中，可能没这句，test就会比较复杂。

![dropout-regularization-implementation.png](https://raw.githubusercontent.com/daiwk/dl.ai/master/c2/imgs/dropout-regularization-implementation.png)

test阶段，**不要使用dropout**，因为我们并不想让预测的结果是random的，或者是有噪音的。如果用了inverted dropout，**在test的时候，就不需要做这个/=keep_prob的操作了。**

![dropout-regularization-test-implementation.png](https://raw.githubusercontent.com/daiwk/dl.ai/master/c2/imgs/dropout-regularization-test-implementation.png)

## understanding dropout

dropout相当于，每个神经元不只仅依赖某一个输入，所以会倾向于将与各输入神经元的连接权重的值分散开来。==>可以shrink squared norm of the weight，类似于L2的效果。

一般为不同的层设置不同的keep_prob，input层一般接近1.0，如果某两层间的矩阵比较大，可以设小一点的keep_prob，例如图中的0.7, 0.5, 0.7。

一般只有在可能overfitting的时候才用dropout，例如cv中，往往输入的像素很多，但数据量没那么大，所以常用dropout来避免过拟合。

downside of dropout：cost function J is no longer well defined...所以每一轮迭代后J的曲线并不一定是单调递减的，一般的做法是，先关闭dropout（keep_prob=1），然后调整模型到J曲线是递减的，再开启dropout，看看dropout有没有引入bug。。

![dropout-regularization-understanding-dropout.png](https://raw.githubusercontent.com/daiwk/dl.ai/master/c2/imgs/dropout-regularization-understanding-dropout.png)

## other regularization methods



# setting up your optimization problem

## normalizing inputs

## vanishing/exploding gradients

## weight initialization for deep networks

## numerical approximation of gradients

## gradient checking

## gradient checking implementation notes



