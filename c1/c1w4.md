contents

  * [deep L-layer neural network](#deep-l-layer-neural-network)
  * [forward propagation in a deep network](#forward-propagation-in-a-deep-network)
  * [getting your matrix dimensions right](#getting-your-matrix-dimensions-right)
  * [why deep representations?](#why-deep-representations)
  * [building blocks of deep neural networks](#building-blocks-of-deep-neural-networks)
  * [forward and backward propagation](#forward-and-backward-propagation)
  * [parameters vs hyperparameters](#parameters-vs-hyperparameters)
  * [what does this have to do with the brain?](#what-does-this-have-to-do-with-the-brain)

## deep L-layer neural network

![deep-neural-network-notation.png](https://raw.githubusercontent.com/daiwk/dl.ai/master/c1/imgs/deep-neural-network-notation.png)

## forward propagation in a deep network

$z^{[l](i)}$表示第l层的第i个训练样本(列向量)，$Z^{[l]}$表示将第l层的这$m$个训练样本全部水平地放在一起形成的矩阵。以此vectorization的方法，可以避免从1->$m$的这个for-loop。

另外，从第1层到第4层这个for-loop是无法避免的。

![forward-propagation-in-a-deep-network.png](https://raw.githubusercontent.com/daiwk/dl.ai/master/c1/imgs/forward-propagation-in-a-deep-network.png)

## getting your matrix dimensions right

![w-and-b-dimension.png](https://raw.githubusercontent.com/daiwk/dl.ai/master/c1/imgs/w-and-b-dimension.png)

vectorized之后，$b^{[l]}$仍然是$(n^{[l]},1)$维，只是因为broadcasting，才复制m遍，变成了$(n^{[l]},m)$维。

![vectorized-implementation.png](https://raw.githubusercontent.com/daiwk/dl.ai/master/c1/imgs/vectorized-implementation.png)

## why deep representations?

![intuition-of-deep-representation.png](https://raw.githubusercontent.com/daiwk/dl.ai/master/c1/imgs/intuition-of-deep-representation.png)

对于$(x_1,x_2,...x_n)$的异或（XOR）操作，如果用树型结构，n个叶子节点，则树深度是$log_2n+1$(深度为k的满二叉树的第i层上有$2^{i-1}$个结点,总共至多有$2^k-1个结点$)，即只需要$O(log_2n)$层的树就能完成。

而如果采用单隐层，则需要$2^{(n-1)}$个节点

![circuit-theory-and-deep-learning.png](https://raw.githubusercontent.com/daiwk/dl.ai/master/c1/imgs/circuit-theory-and-deep-learning.png)

## building blocks of deep neural networks

forward时，需要cache $Z^{[l]}$以供backward使用。

![forward-and-backward-functions.png](https://raw.githubusercontent.com/daiwk/dl.ai/master/c1/imgs/forward-and-backward-functions.png)

为了计算backward，其实需要cache的有$Z^{[l]}$、$W^{[l]}$以及$b^{[l]}$：

![forward-backward-functions.png](https://raw.githubusercontent.com/daiwk/dl.ai/master/c1/imgs/forward-backward-functions.png)

## forward and backward propagation

forward propagation for layer l:

![forward-propagation-for-layer-l.png](https://raw.githubusercontent.com/daiwk/dl.ai/master/c1/imgs/forward-propagation-for-layer-l.png)




## parameters vs hyperparameters

## what does this have to do with the brain?


