contents

<!-- TOC -->

- [1. hyperparameter tuning](#1-hyperparameter-tuning)
    - [1.1. tuning process](#11-tuning-process)
    - [1.2. using an appropriate scale to pick hyperparameters](#12-using-an-appropriate-scale-to-pick-hyperparameters)
    - [1.3. hyperparameters tuning in practice: Pandas vs. Caviar](#13-hyperparameters-tuning-in-practice-pandas-vs-caviar)
- [2. batch normalization](#2-batch-normalization)
    - [2.1. normalizing activations in a network](#21-normalizing-activations-in-a-network)
    - [2.2. fitting batch norm into a neural network](#22-fitting-batch-norm-into-a-neural-network)
    - [2.3. why does batch norm work?](#23-why-does-batch-norm-work)
    - [2.4. batch norm at test time](#24-batch-norm-at-test-time)
- [3. multi-class classification](#3-multi-class-classification)
    - [3.1. softmax regression](#31-softmax-regression)
    - [3.2. training a softmax classifier](#32-training-a-softmax-classifier)
- [4. introduction to programming frameworks](#4-introduction-to-programming-frameworks)
    - [4.1. deep learning frameworks](#41-deep-learning-frameworks)
    - [4.2. tensorflow](#42-tensorflow)

<!-- /TOC -->

# 1. hyperparameter tuning

## 1.1. tuning process

如图，各超参的优先级为红>黄>紫，而adam的三个参数基本都不调整。

![hyperparameters.png](https://raw.githubusercontent.com/daiwk/dl.ai/master/c2/imgs/hyperparameters.png)

当参数比较少时，可以用表格，两两组合去尝试，但如果超参很多，可以用随机组合，因为事先并不知道各个参数的重新程度。例如，用一个很重要的参数$\alpha$和一个没那么重要的参数$\epsilon$组合，可能会发现试完25种组合，不论$\epsilon$取什么值，效果都基本只和$\alpha$的取值有关。而如果用随机组合的方法，同样的25个模型，可以试25种$\alpha$的取值。

![tuning-process-try-random-values.png](https://raw.githubusercontent.com/daiwk/dl.ai/master/c2/imgs/tuning-process-try-random-values.png)

coarse to fine(由粗到精)原则：当发现某个空间里的超参组合表现比较好时，例如图中蓝圈的那几个点，那么，可以在一个小区域（图中的蓝色正方形）内sample更多的参数组合。

![tuning-process-coarse-to-fine.png](https://raw.githubusercontent.com/daiwk/dl.ai/master/c2/imgs/tuning-process-coarse-to-fine.png)

## 1.2. using an appropriate scale to pick hyperparameters

一种方法是根据均匀分布(uniform distribution)来随机地选参数值：

![pick-hyperparameters-uniformly.png](https://raw.githubusercontent.com/daiwk/dl.ai/master/c2/imgs/pick-hyperparameters-uniformly.png)

但，如果$\alpha$从0.0001到1之间均匀地随机采样，会有90%的值落在0.1-1之间，而0.001到0.1之间只有10%的值。相当于用了90%的资源搜索0.1-1的值，但只用10%的资源搜索0.0001-0.1的值。

更合理的方法是，在log scale上进行搜索，如图中的第二个数轴，每个区域的起点和终点是10倍的关系。

起点是$10^a$，终点是$10^b$，而$0.0001=10^a$，所以，$a=log_{10}0.0001=-4$。所以

```python
r = -4 * np.random.rand() # -4 <= r <= 0
alpha = 10 ** r # 10^-4 <= alpha <= 1
```

![pick-hyperparameters-log-scale.png](https://raw.githubusercontent.com/daiwk/dl.ai/master/c2/imgs/pick-hyperparameters-log-scale.png)

而对于exponentially weight average的超参$\beta$，取值范围从0.9(相当于10天的平均)，到0.999（相当于1000天的平均）。这种情况，就做一个变换，对$(1-\beta)$在log scale上进行采样就行了。

特别地，**对于exponentially weight average而言，不能使用简单的均匀采样，要用log scale的采样。**因为，**当$\beta$的取值趋近于1时，$\beta$的微小改变，会导致结果特别敏感**。取$\beta$时，相当于$1/(1-\beta)$天的平均：
+ 当$\beta$比较小时（如0.9000变化成0.9005），这个变化就是从10.0天到10.05的变化
+ 而$\beta$比较大时（如0.9990变化成0.999），那这个变化就是从1/(1-0.999)=1000天变到1/(1-0.9995)=2000天了！

![pick-hyperparameters-exponetially-weighted-average.png](https://raw.githubusercontent.com/daiwk/dl.ai/master/c2/imgs/pick-hyperparameters-exponetially-weighted-average.png)

## 1.3. hyperparameters tuning in practice: Pandas vs. Caviar

# 2. batch normalization

## 2.1. normalizing activations in a network

## 2.2. fitting batch norm into a neural network

## 2.3. why does batch norm work?

## 2.4. batch norm at test time

# 3. multi-class classification

## 3.1. softmax regression

## 3.2. training a softmax classifier

# 4. introduction to programming frameworks

## 4.1. deep learning frameworks

## 4.2. tensorflow

