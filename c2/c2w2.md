contents

  * [mini-batch gradient descent](#mini-batch-gradient-descent)
  * [understanding mini-batch gradient descent](#understanding-mini-batch-gradient-descent)
  * [exponentially weighted averages](#exponentially-weighted-averages)
  * [understanding exponentially weighted averages](#understanding-exponentially-weighted-averages)
  * [bias correction in exponentially weighted averages](#bias-correction-in-exponentially-weighted-averages)
  * [gradient descent with momentum](#gradient-descent-with-momentum)
  * [RMSprop](#rmsprop)
  * [Adam optimization algorithm](#adam-optimization-algorithm)
  * [learning rate decay](#learning-rate-decay)
  * [the problem of local optima](#the-problem-of-local-optima)

## mini-batch gradient descent

+ batch gradient descent：遍历所有训练样本
+ mini-batch gradient descent：一次处理一个min-batch

+ $X^{(i)}$表示第i个训练样本
+ $z^{[l]}$表求第l层的激活函数的输入
+ $X^{{t}},Y^{{t}}$表示第t个mini-batch的样本

![mini-batch-gradient-descent-introduction.png](https://raw.githubusercontent.com/daiwk/dl.ai/master/c2/imgs/mini-batch-gradient-descent-introduction.png)

假设有5000000个样本，那么在一个epoch中，遍历5000个mini-batch，每个mini-batch（1000个样本）中，先前向计算，然后算这1000个样本的loss，然后反向传播。

注意：一个mini-batch就反射传播一次，更新一次参数，所以一个epoch就更新了5000次参数。

![mini-batch-gradient-descent.png](https://raw.githubusercontent.com/daiwk/dl.ai/master/c2/imgs/mini-batch-gradient-descent.png)

## understanding mini-batch gradient descent

## exponentially weighted averages

## understanding exponentially weighted averages

## bias correction in exponentially weighted averages

## gradient descent with momentum

## RMSprop

## Adam optimization algorithm

## learning rate decay

## the problem of local optima


