contents

<!-- TOC -->

- [1. shallow neural network](#1-shallow-neural-network)
  - [1.1. neural networks overview](#11-neural-networks-overview)
  - [1.2. neural network representation](#12-neural-network-representation)
  - [1.3. computing a neural network's output](#13-computing-a-neural-networks-output)
  - [1.4. vectorizing across multiple examples](#14-vectorizing-across-multiple-examples)
  - [1.5. explanation for vectorized implementation](#15-explanation-for-vectorized-implementation)
  - [1.6. activation functions](#16-activation-functions)
  - [1.7. why do you need non-linear activation functions](#17-why-do-you-need-non-linear-activation-functions)
  - [1.8. derivatives of activation functions](#18-derivatives-of-activation-functions)
  - [1.9. gradient descent for neural networks](#19-gradient-descent-for-neural-networks)
  - [1.10. backpropagation intuition](#110-backpropagation-intuition)
  - [1.11. random initialization](#111-random-initialization)

<!-- /TOC -->

# 1. shallow neural network

## 1.1. neural networks overview

其中，每个神经元完成了$z=w^Tx+b$以及$a=\sigma (z)$两个操作($a$表示activation)，每一层的数据用上标[i]表示。

![neural-networks-overview](../c1/imgs/neural-networks-overview.png)

## 1.2. neural network representation

图示是一个2层nn（inputlayer不算在内，有hidden和output两层）。

如果输入的x有3维，在lr中，$shape(w)=(1,3)$，$shape(b)=(1,1)$。

而在nn中，$shape(w^{[1]})=(4,3)$因为有4个神经元，输入是3维。同理$shape(b^{[1]})=(4,1)$。

而$shape(w^{[2]})=(1,4)$，因为只有1个神经元，输入是3维。同理$shape(b^{[2]})=(1,1)$。

![neural-network-representation](../c1/imgs/neural-network-representation.png)

## 1.3. computing a neural network's output

![compute-nn-output-1](../c1/imgs/compute-nn-output-1.png)
![compute-nn-output-1](../c1/imgs/compute-nn-output-2.png)

## 1.4. vectorizing across multiple examples

![vectorizing-across-multiple-examples-for-loop](../c1/imgs/vectorizing-across-multiple-examples-for-loop.png)

矩阵$X$纵向是x的维数（行数），横向是training examples的个数（列数）。

矩阵$Z$、$A$纵向是hidden units的个数（行数），横向是training examples的个数（列数）。

![vectorizing-across-multiple-examples-vectorization](../c1/imgs/vectorizing-across-multiple-examples-vectorization.png)

## 1.5. explanation for vectorized implementation

![justification-for-vectorized-implementation](../c1/imgs/justification-for-vectorized-implementation.png)
![vectorizing-across-multiple-examples-recap](../c1/imgs/vectorizing-across-multiple-examples-recap.png)

## 1.6. activation functions

一般来说，$tanh$效果比$sigmoid$好，因为均值是0。但对于outputlayer而言，一般$y \in \{0,1\}$，所以希望$0\le \hat y \le 1$，所以会用$sigmoid$。

$ReLU(z)=max(0,z)$比$tanh$好，因为当$x\le 0$时，梯度为0。而$leaky ReLU$在$x\le 0$时，梯度是接近0，效果会好一点，但在实践中还是$ReLU$居多。当$x>0$时，梯度和$x\le0$差很多，所以训练速度会加快。

![activation-functions](../c1/imgs/activation-functions.png)

## 1.7. why do you need non-linear activation functions

linear activation: 因为$z=wx+b$，所以激活函数$g(z)=z=wx+b$就叫linear activation function，也叫identity activatioin function。

不要在hidden layer用linear activation functions，因为多个linear嵌套，实质上还是linear。

例外，当进行回归时，$y\in R$，可以hidden layer用$ReLU$，但output layer用linear activation。

![why-use-non-linear-functions.png](../c1/imgs/why-use-non-linear-functions.png)

## 1.8. derivatives of activation functions

sigmoid的导数

![derivative-of-sigmoid.png](../c1/imgs/derivative-of-sigmoid.png)

tanh的导数

![derivative-of-tanh.png](../c1/imgs/derivative-of-tanh.png)

relu和leaky relu的导数(z=0时不可导，但在工程上，直接归入z>0部分)

![derivative-of-tanh.png](../c1/imgs/derivative-of-tanh.png)

## 1.9. gradient descent for neural networks

**记住每个W/b的shape!**

![gradient-descent-for-neural-networks](../c1/imgs/gradient-descent-for-neural-networks.png)

其中的keepdims=True表示，输出的$shape=(n^{[2]},1)$而非$(n^{[2]},)$
另外求dz时，两项之前是element-wise product(np.multiply)，其中第二项就是对激活函数求在$z^{[1]}$的导数

![forward-propagation-and-back-propagation](../c1/imgs/forward-propagation-and-back-propagation.png)

## 1.10. backpropagation intuition

先回顾一下lr：

![back-propagation-lr](../c1/imgs/back-propagation-lr.png)

然后看nn：

![back-propagation-nn](../c1/imgs/back-propagation-nn.png)

扩展到m个examples，并进行vectorize：

![back-propagation-nn-vectorized](../c1/imgs/back-propagation-nn-vectorized.png)
![back-propagation-nn-vectorized-all](../c1/imgs/back-propagation-nn-vectorized-all.png)

**问：为何有的有1/m，有的没有。。。。。**

## 1.11. random initialization

lr的训练，参数一般都初始化为0。但nn，如果初始化为0，会发现算出来的$a^{[1]}_1=a^{[1]}_2$，$dz^{[1]}_1=dz^{[1]}_2$，所以相当于每个神经元的influence是一样的(symetric)，所以$dw^{[1]}$这个矩阵的每一行都相等。
"Each neuron in the first hidden layer will perform the same computation. So even after multiple iterations of gradient descent each neuron in the layer will be computing the same thing as other neurons."

![if-initialize-zero](../c1/imgs/if-initialize-zero.png)

解决：随机初始化
只要w随机初始化了，b其实影响不大，0就可以了。如果用的是sigmoid/tanh的话，随机初始化时，尽量小，因为如果大的话，激活后会接近两端(无论是+无穷还是-无穷，梯度都接近0)，导致学习过程变得很慢。**对于浅层的网络，0.01就可以了，但如果是更深的网络，可能需要其他系数，这个选择过程，在后面的课程会讲。。。**

![random-initialization](../c1/imgs/random-initialization.png)


