contents

  * [neural networks overview](#neural-networks-overview)
  * [neural network representation](#neural-network-representation)
  * [computing a neural network's output](#computing-a-neural-networks-output)
  * [vectorizing across multiple examples](#vectorizing-across-multiple-examples)
  * [explanation for vectorized implementation](#explanation-for-vectorized-implementation)
  * [activation functions](#activation-functions)
  * [why do you need non-linear activation functions](#why-do-you-need-non-linear-activation-functions)
  * [derivatives of activation functions](#derivatives-of-activation-functions)
  * [gradient descent for neural networks](#gradient-descent-for-neural-networks)
  * [backpropagation intuition](#backpropagation-intuition)
  * [random initialization](#random-initialization)


## neural networks overview

其中，每个神经元完成了$z=w^Tx+b$以及$a=\sigma (z)$两个操作($a$表示activation)，每一层的数据用上标[i]表示。

![neural-networks-overview](https://raw.githubusercontent.com/daiwk/dl.ai/master/c1/imgs/neural-networks-overview.png)

## neural network representation

图示是一个2层nn（inputlayer不算在内，有hidden和output两层）。

如果输入的x有3维，在lr中，$shape(w)=(1,3)$，$shape(b)=(1,1)$。

而在nn中，$shape(w^{[1]})=(4,3)$因为有4个神经元，输入是3维。同理$shape(b^{[1]})=(4,1)$。

而$shape(w^{[2]})=(1,4)$，因为只有1个神经元，输入是3维。同理$shape(b^{[2]})=(1,1)$。

![neural-network-representation](https://raw.githubusercontent.com/daiwk/dl.ai/master/c1/imgs/neural-network-representation.png)

## computing a neural network's output

![compute-nn-output-1](https://raw.githubusercontent.com/daiwk/dl.ai/master/c1/imgs/compute-nn-output-1.png)
![compute-nn-output-1](https://raw.githubusercontent.com/daiwk/dl.ai/master/c1/imgs/compute-nn-output-2.png)

## vectorizing across multiple examples

![vectorizing-across-multiple-examples-for-loop](https://raw.githubusercontent.com/daiwk/dl.ai/master/c1/imgs/vectorizing-across-multiple-examples-for-loop.png)

矩阵$X$纵向是x的维数（行数），横向是training examples的个数（列数）。

矩阵$Z$、$A$纵向是hidden units的个数（行数），横向是training examples的个数（列数）。

![vectorizing-across-multiple-examples-vectorization](https://raw.githubusercontent.com/daiwk/dl.ai/master/c1/imgs/vectorizing-across-multiple-examples-vectorization.png)

## explanation for vectorized implementation

![justification-for-vectorized-implementation](https://raw.githubusercontent.com/daiwk/dl.ai/master/c1/imgs/justification-for-vectorized-implementation.png)
![vectorizing-across-multiple-examples-recap](https://raw.githubusercontent.com/daiwk/dl.ai/master/c1/imgs/vectorizing-across-multiple-examples-recap.png)

## activation functions

一般来说，$tanh$效果比$sigmoid$好，因为均值是0。但对于outputlayer而言，一般$y \in \\{0,1\\}$，所以希望$0\le \hat y \le 1$，所以会用$sigmoid$。

$ReLU(z)=max(0,z)$比$tanh$好，因为当$x\le 0$时，梯度为0。而$leaky ReLU$在$x\le 0$时，梯度是接近0，效果会好一点，但在实践中还是$ReLU$居多。当$x>0$时，梯度和$x\le0$差很多，所以训练速度会加快。

![activation-functions](https://raw.githubusercontent.com/daiwk/dl.ai/master/c1/imgs/activation-functions.png)

## why do you need non-linear activation functions

linear activation: 因为$z=wx+b$，所以激活函数$g(z)=z=wx+b$就叫linear activation function，也叫identity activatioin function。

不要在hidden layer用linear activation functions，因为多个linear嵌套，实质上还是linear。

例外，当进行回归时，$y\in R$，可以hidden layer用$ReLU$，但output layer用linear activation。

![why-use-non-linear-functions.png](https://raw.githubusercontent.com/daiwk/dl.ai/master/c1/imgs/why-use-non-linear-functions.png)

## derivatives of activation functions

sigmoid的导数
![derivative-of-sigmoid.png](https://raw.githubusercontent.com/daiwk/dl.ai/master/c1/imgs/derivative-of-sigmoid.png)

tanh的导数
![derivative-of-tanh.png](https://raw.githubusercontent.com/daiwk/dl.ai/master/c1/imgs/derivative-of-tanh.png)

relu和leaky relu的导数(z=0时不可导，但在工程上，直接归入z>0部分)
![derivative-of-tanh.png](https://raw.githubusercontent.com/daiwk/dl.ai/master/c1/imgs/derivative-of-tanh.png)

## gradient descent for neural networks

**记住每个W/b的shape!**
![gradient-descent-for-neural-networks](https://raw.githubusercontent.com/daiwk/dl.ai/master/c1/imgs/gradient-descent-for-neural-networks.png)

其中的keepdims=True表示，输出的$shape=(n^{[2]},1)$而非$(n^{[2]},)$
另外求dz时，两项之前是element-wise product，其中第二项就是对激活函数求在$z^{[1]}$的导数
![forward-propagation-and-back-propagation](https://raw.githubusercontent.com/daiwk/dl.ai/master/c1/imgs/forward-propagation-and-back-propagation.png)

## backpropagation intuition

先回顾一下lr：
![back-propagation-lr](https://raw.githubusercontent.com/daiwk/dl.ai/master/c1/imgs/back-propagation-lr.png)

然后看nn：
![back-propagation-nn](https://raw.githubusercontent.com/daiwk/dl.ai/master/c1/imgs/back-propagation-nn.png)

扩展到m个examples，并进行vectorize：
![back-propagation-nn-vectorized](https://raw.githubusercontent.com/daiwk/dl.ai/master/c1/imgs/back-propagation-nn-vectorized.png)
![back-propagation-nn-vectorized-all](https://raw.githubusercontent.com/daiwk/dl.ai/master/c1/imgs/back-propagation-nn-vectorized.png-all.png)
[问：为何有的有1/m，有的没有。。]

## random initialization


